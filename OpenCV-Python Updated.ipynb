{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T05:56:41.948609Z",
     "start_time": "2021-09-03T05:56:41.897940Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "cv2.__version__\n",
    "img=cv2.imread(r\"C:\\Users\\JokeRR\\Referances\\python_projects\\Number-plate Detection\\image3.jpg\") #,cv2.IMREAD_GRAYSCALE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gui Features: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Getting Started with Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T05:51:07.749754Z",
     "start_time": "2021-09-02T05:51:07.725294Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Read Image :\n",
    "img=cv2.imread(r\"C:\\Users\\JokeRR\\Referances\\python_projects\\Number-plate Detection\\image3.jpg\",cv2.IMREAD_COLOR) \n",
    "\n",
    "# Color channels \n",
    "cv2.IMREAD_COLOR|(1) : Colored\n",
    "cv2.IMREAD_GRAYSCALE|(0) : Gray-scale\n",
    "cv2.IMREAD_UNCHANGED|(-1) : Alpha channel\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-02T05:51:08.195Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Window Size:\n",
    "cv2.namedWindow('window_name',cv2.WINDOW_NORMAL)   # Resizable window\n",
    "cv2.namedWindow('window_name',cv2.WINDOW_AUTOSIZE) # Auto-Sized window\n",
    "\n",
    "\n",
    "## Display Image: Using cv2.imshow()\n",
    "cv2.imshow('image_name2',img)  # Show image  @imshow('window_name',image)\n",
    "cv2.waitKey(0)                 # Wait to End:Keyboard binding @cv2.waitKey(any key)\n",
    "cv2.destroyAllWindows()        # Destroy all image windows\n",
    "\n",
    "## Using Matplotlib :\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img,cmap='gray',interpolation='bicubic')# show Image\n",
    "plt.xticks([]),plt.yticks([])                      # hide tick values on X & Y\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-02T05:51:09.170Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Write Image:\n",
    "cv2.imread(r'C:\\Users\\joker',1)   #@ cv2.imread(path,color_mode) \n",
    "\n",
    "\n",
    "# Save when button pressed:\n",
    "cv2.imshow(\"Display window\",img)\n",
    "k=cv2.waitKey(0)\n",
    "if k==ord(\"s\"):\n",
    "    cv2.imwrite(\"starry_night.png\",img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-02T05:51:09.872Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Basic image properties\n",
    "img.size    # Size of image(Total Pixel Size)\n",
    "img.shape   # Shape of image\n",
    "img.dtype   # data type of image\n",
    "img[11,433] # Access Image Pixel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Getting Started with Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Drawing Shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Line \"\"\" \n",
    "Draws Line between pt1 & pt2\n",
    "@ line(img,pt1,pt2,color,thickness,lineType,shift)\n",
    " * img: Image.\n",
    " * pt1: 1st point of line ; pt2: 2nd point of line\n",
    " * color: Line color\n",
    " * thickness: Line thickness.\n",
    " * lineType: Type of the line\n",
    " * shift: Number of fractional bits in point coordinates.\n",
    "\n",
    "cv2.line(img,(100,250),(300,400),(255,0,255),3) \n",
    "\n",
    "\n",
    "\"\"\" Polygon \"\"\"\n",
    "Draws one or more polygonal curves.\n",
    "@ polylines(img,pts,isClosed,color,thickness,lineType,shift)\n",
    " * pts: Array of polygonal curves.\n",
    " * isClosed: whether drawn polylines are closed or not\n",
    " * shift: Number of fractional bits in vertex coordinates\n",
    "\n",
    "coordinates=np.array([[10,5],[20,30],[70,20],[50,10]],np.int32).reshape((-1,1,2))  ## set vertices\n",
    "cv2.polylines(img,[coordinates],True,(0,255,255))\n",
    "\n",
    "\n",
    "\"\"\" Circle \"\"\"\n",
    "Draws simple|filled circle with given center & radius\n",
    "@ circle(img, center, radius, color[, thickness[, lineType[, shift]]]) -> img\n",
    " * center: Center of the circle.\n",
    " * radius: Radius of the circle.\n",
    " * shift: Number of fractional bits in coordinates of center & in radius value.\n",
    "\n",
    "cv.circle(img,(415,215),20,(0,0,255),1)\n",
    "\n",
    "\n",
    "\"\"\" Ellipse \"\"\"\n",
    "Draws simple|thick elliptic arc|fills ellipse sector\n",
    "@ ellipse(img,center,axes,angle,startAngle,endAngle,color,thickness,lin * \n",
    " * center: Center of ellipse\n",
    " * axes: Half of size of ellipse main axes\n",
    " * angle: Ellipse rotation angle in degrees\n",
    " * startAngle: Starting angle of elliptic arc in degrees.\n",
    " * endAngle: Ending angle of elliptic arc in degrees.\n",
    " * shift: Number of fractional bits in coordinates of center & values of axes.\n",
    "    \n",
    "cv2.ellipse(img,(256,256),(100,50),0,0,180,(0,0,255),-1)\n",
    "\n",
    "\n",
    "\"\"\" Rectangle \"\"\"\n",
    "draws outline|filled rectangle whose two opposite corners are pt1 & pt2\n",
    "@ rectangle(img,pt1,pt2,color,thickness,lineType,shift)\n",
    " * pt1: Vertex of rectangle ; pt2: Vertex of rectangle opposite to pt1 \n",
    " * shift: Number of fractional bits in the point coordinates.\n",
    "    \n",
    "cv.rectangle(img,(0,0),(img.shape[1]//2,img.shape[0]//2),(0,255,0),-1)\n",
    "\n",
    "\n",
    "\"\"\" Text \"\"\"\n",
    "Draws a text string\n",
    "@ putText(img,text,org,fontFace,fontScale,color,thickness,lineType,bottomLeftOrigin)\n",
    " * text: Text string to be drawn.\n",
    " * org: Bottom-left corner of text string in image.\n",
    " * fontFace: Font type\n",
    " * fontScale: Font scale factor (multiplied by font-specific base size)\n",
    " * bottomLeftOrigin: True: image data origin is at bottom-left corner; False: top-left corner\n",
    "\n",
    "text=\"LoL !!!!!! \"\n",
    "cv.putText(img,text,(0,225),cv.FONT_HERSHEY_TRIPLEX,1.0,(0,255,0),2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Mouse as Paint-Brush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Trackbar as the Color Palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Core Operations :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Basic Operations on images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T15:59:57.850643Z",
     "start_time": "2021-09-02T15:59:57.834627Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Access & Modify  Pixel & color-channel values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Access & Modify  Pixel & color-channel values : For Colored-image\n",
    "\"\"\" Access Pixel Values \"\"\" Pixel-values accessed by its 'row' & 'column'\n",
    "img[100,100]           # Intensity of [Blue,Red,Green]  @ image[row,col]\n",
    "\n",
    "  \n",
    "\"\"\" Access Colored-values \"\"\"  Specific color-channelel value(Blue,Green,Red) can be Accessed\n",
    "channel={0:Blue|1:green|2:Red} \n",
    "\n",
    "img[100,100,0]        # Blue-channel value @ img[row,col,channel]  \n",
    "img.item(100,100,0)   # Blue-channel value @ img.item(row,col,channel)\n",
    "\n",
    "\n",
    "\"\"\" Modify Pixel Value \"\"\" Modify pixel-value\n",
    "img[100,100]=[220,255,100]    # Modify pixel-values @ img[row,column]=[Blue_value,Green_value,Red_value]\n",
    "\n",
    "\n",
    "\"\"\" Modify Colored-values \"\"\" Modifyy specific color-channelel value(Blue,Green,Red)\n",
    "channel={0:Blue|1:green|2:Red} \n",
    "\n",
    "img[100,100,0]=233           # Modify Blue-channel value @ img[row,col,channel]=value  \n",
    "img.item(100,100,0)=233      # Modify Blue-channel value @ img.item(row,col,channel)=value\n",
    "img.itemset((10,10,0),211)   # Modify Blue-channel value @ img.itemset((row,col,chanel),value)\n",
    "\n",
    "\n",
    "### Access & Modify  Pixel & color-channel values : For Gray-Scaled image\n",
    "\"\"\" Access Pixel Values \"\"\" Pixel-values accessed by its 'row' & 'column'\n",
    "img[100,100]           # Access pixel-value  @ image[row,col]\n",
    "img.item(100,100)      # Access pixel-value  @ image.item[row,col]\n",
    "\n",
    "\"\"\" Modify Pixel Value \"\"\" Modify pixel-value\n",
    "img[100,100]=220           # Modify pixel value @ img[row,column]=value\n",
    "img.item(100,100)          # Modify pixel value @ image.item[row,col]=value\n",
    "img.itemset((10,10),211)   # Modify pixel value @ img.itemset((row,col),value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Set Region of Image (ROI) :\n",
    "* ROI: `Refers to choosing specific Region of Image,that obtained using 'Numpy Indexing'`\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T16:34:21.156063Z",
     "start_time": "2021-09-02T16:34:16.033334Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Selecting Region:\n",
    "region=img[200:300,50:200]  # @ region=img[start_row:end_row,start_col:end_col]\n",
    "\n",
    "## Adding Region:\n",
    "img[:240,100:260]=region    # @ img[start_row:end_row,start_col:end_col]=region\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Split & Merge Color channel :\n",
    "***cv2.split() :*** `split color channels`\n",
    "\n",
    "***cv2.merge() :*** `Merge split-colored channels`\n",
    "\n",
    "`cv2.split() & cv2.merge() are very time taking(we can use alternative)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Split Color-channels \"\"\"\n",
    "@ split(m,mv)\n",
    " * m: i/p multi-channel array\n",
    " * mv: o/p vector of arrays (arrays reallocated if needed)\n",
    "\n",
    "B,G,R=cv2.split(img)\n",
    "# or\n",
    "blue=img[:,:,0]\n",
    "green=img[:,0,:]\n",
    "red=img[0,:,:]\n",
    "\n",
    "\n",
    "\"\"\" Merge color-Channel \"\"\"\n",
    "@ merge(mv,dst)\n",
    " * mv: i/p vector of matrices to be merged (all matrices must same size & same depth)\n",
    " * dst: o/p array of same size & same depth as mv[0] (No. of channels = total number of channels in matrix array)\n",
    "\n",
    "merged=cv2.merge([B,G,R])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Image Border :\n",
    "* We can add border of image(like photo frame) , zero padding etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@ cv2.copyMakeBorder(src,top,bottom,left,right,borderType,value)\n",
    " * src:Input image(where to draw border)\n",
    " * top,bottom,left,right: border width(in number of pixel)\n",
    " * borderType: Border type\n",
    "    cv2.BORDER_CONSTANT - Constant colored border(value=color)\n",
    "    cv2.BORDER_REFLECT - Mirror reflection of border elements (fedcba|abcdefgh|hgfedcb)\n",
    "    cv2.BORDER_REFLECT_101 | cv2.BORDER_DEFAULT - Same as above, but with a slight change, like this : gfedcb|abcdefgh|gfedcba\n",
    "    cv2.BORDER_REPLICATE - Last element is replicated throughout, like this: aaaaaa|abcdefgh|hhhhhhh\n",
    "    cv2.BORDER_WRAP -it will look like this (cdefgh|abcdefgh|abcdefg)\n",
    " * value:color value for cv2.BORDER_CONSTANT ([255,0,0]=B|[0,255,0]=G|[0,0,255]=R)\n",
    "\n",
    "\n",
    "border=cv2.copyMakeBorder(img,10,10,10,10,cv2.BORDER_CONSTANT,value=[255,0,0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Arithmetic Operations on Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Image Addition \"\"\" Adds multiple images or scalar value\n",
    "Note: Imges must be of 'same size','same type' & 'same depth'\n",
    "\n",
    "@ add(src1,src2,dst,mask,dtype)\n",
    " * src1: 1st input array|scalar ; src2: 2nd input array|scalar.\n",
    " * dst: o/p array (same size & no. of channels as i/p array; depth: defined by dtype or src1/src2)\n",
    " ** mask: specifies elements of o/p array to be changed (mask: 8-bit single channel array)\n",
    " ** dtype:depth of o/p array \n",
    "\n",
    "var=cv2.add(img1,img2)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Image Blending \"\"\" \n",
    "- Image addition but 'different weights'can be given,that controlls blending(transparency) \n",
    "- Calculates weighted sum of two images \n",
    "Formula: added=ima1.alpha+img2.beta+gamma\n",
    "\n",
    "@ cv2.addWeighted(src1,alpha,src2,beta,gamma) \n",
    " * alpha: weight for 1st image, perform a cool transition (range from 0-1)\n",
    " * beta: weight for 2st image, perform a cool transition (range from 0-1) \n",
    " * gamma: weight/scalar added to blend,controls Exposure \n",
    "    \n",
    "dst=cv2.addWeighted(img,0.4,img1,0.6,0)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Bitwise Operations \"\"\" AND, OR, NOT & XOR operations   \n",
    "- Useful for 'extracting any part of image','defining' & 'working with non-rectangular ROI'\n",
    "\n",
    "rectangle=cv.rectangle(blank.copy(),(30,30),(370,370),255,-1)\n",
    "circle=cv.circle(blank.copy(),(200,200),200,255,-1)\n",
    "\n",
    "## bitwise AND --> intersecting regions\n",
    "bitwise_and=cv.bitwise_and(rectangle,circle)\n",
    "\n",
    "## bitwise OR --> non-intersecting and intersecting regions\n",
    "bitwise_or=cv.bitwise_or(rectangle,circle)\n",
    "\n",
    "## bitwise XOR --> non-intersecting regions\n",
    "bitwise_xor=cv.bitwise_xor(rectangle,circle)\n",
    "\n",
    "## bitwise NOT\n",
    "bitwise_not=cv.bitwise_not(circle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Performance Measurement and Improvement Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "https://docs.opencv.org/4.5.2/d7/d16/tutorial_py_table_of_contents_core.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Processing :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Changing Colorspaces :\n",
    " * `Converts images from one color-space to another(BGR -> Gray, BGR -> HSV etc.)`\n",
    " * `OpenCV supports 150 color-space conversion methods available in OpenCV `\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Color  Spaces & Ranges \"\"\":\n",
    "- In Color Images(BGR) -> [Blue,Green,Red] (all ranges from [0,255])\n",
    "- In HSV -> [Hue,Saturation,Value] (h range [0,179] & s,v range[0,255])\n",
    "- In Gray -> [Shades of gray] (Ranges[0-255])\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" List of Color-Conversion Flags \"\"\"\n",
    "flags=[i for i in dir(cv2) if i.startswith('COLOR_')]\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Changing color space \"\"\": convert images from one color space to another(BGR -> Gray|BGR -> HSV)\n",
    "@ cvtColor(src,code,dst,dstCn)\n",
    " * src: I/p image '8-bit unsigned','16-bit unsigned','single-precision floating-point'\n",
    " * dst: O/p image of same size & depth as src\n",
    " * code: color space conversion code|flags\n",
    " * dstCn: No. of channels in destination image  if 0:number of channels is derived automatically from src & code\n",
    "    \n",
    "gray=cv.cvtColor(img,cv.COLOR_BGR2GRAY)   # BGR to Grayscale\n",
    "hsv=cv.cvtColor(img,cv.COLOR_BGR2HSV)     # BGR to HSV\n",
    "lab=cv.cvtColor(img,cv.COLOR_BGR2LAB)     # BGR to L*a*b\n",
    "rgb=cv.cvtColor(img,cv.COLOR_BGR2RGB)     # BGR to RGB\n",
    "lab_bgr=cv.cvtColor(lab,cv.COLOR_LAB2BGR) # HSV to BGR\n",
    "# can't  convert gray-scale to HSV directly: we have to convert gray->BGR->HSV \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Object Tracking \n",
    "https://docs.opencv.org/4.5.2/df/d9d/tutorial_py_colorspaces.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Geometric Transformations of Images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Scaling :\n",
    "* Used for 'Resizing image'\n",
    "* Size of image can be specified manually or scaling factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@ cv2.resize(img,,fx,fy,interpolation)\n",
    " * fx,fy :scale along x & y direction\n",
    " * interpolation: type of scaling\n",
    "     cv2.INTER_AREA: Shrink \n",
    "     cv2.INTER_CUBIC: Slow\n",
    "     cv2.INTER_LINEAR: Zooming\n",
    "\n",
    "scale=cv2.resize(img,None,fx=2,fy=2,interpolation=cv2.INTER_CUBIC)    \n",
    "#or\n",
    "height,width=img.shape[:2]\n",
    "res=cv2.resize(img,(2*width,2*height),interpolation=cv2.INTER_AREA)\n",
    "#or\n",
    "res=cv.resize(img,(500,500),interpolation=cv.INTER_CUBIC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T06:19:09.704516Z",
     "start_time": "2021-09-03T06:19:09.691628Z"
    },
    "hidden": true
   },
   "source": [
    "### Translation :\n",
    " - Shifting object Location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### warpAffine:Takes 2x3 transformation matrix as input\n",
    "@ warpAffine(src,M,dsize,dst,flags,borderMode,borderValue)\n",
    " * src: input image\n",
    " * dst: output image (size dsize & same type as src) \n",
    " * M: \\f$2\\times 3\\f$ transformation matrix.\n",
    " * dsize: size of output image.\n",
    " * flags: combination of interpolation methods\n",
    " * borderMode: pixel extrapolation method\n",
    "    borderMode=BORDER_TRANSPARENT: pixels in destination image corresponding to \"outliers\" in source image are not modified by function\n",
    " * borderValue: value used in case of constant border (default=0)\n",
    "    \n",
    "row,col=img.shape[:2]\n",
    "x,y=-100,100\n",
    "transMat=np.float32([[1,0,x],[0,1,y]])\n",
    "translated=cv2.warpAffine(img,transMat,(col,row))\n",
    "\n",
    "\n",
    "### warpPerspective: Takes 3x3 transformation matrix as input\n",
    "https://docs.opencv.org/4.5.2/da/d6e/tutorial_py_geometric_transformations.html\n",
    "\n",
    "    \n",
    "### Positions:\n",
    "   -x --> Left  # -y --> Up\n",
    "   x --> Right  # y --> Down\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Rotation :\n",
    "- Rotate an image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@ getRotationMatrix2D(center,angle,scale)\n",
    " * center: Center of rotation in source image\n",
    " * angle: Rotation angle in degrees.\n",
    "    +ve values: counter-clockwise rotation (coordinate origin is assumed to be top-left corner)\n",
    " * scale: Isotropic scale factor\n",
    "    \n",
    "row,col=img.shape[:2]\n",
    "rotate=cv2.getRotationMatrix2D(center=(cols/2.0,rows/2.0),angle=90,scale=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Affine Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T06:41:27.009227Z",
     "start_time": "2021-09-03T06:41:26.988800Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function getAffineTransform:\n",
      "\n",
      "getAffineTransform(...)\n",
      "    getAffineTransform(src, dst) -> retval\n",
      "    .   @overload\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@ getAffineTransform(src,dst)\n",
    " * src: input image\n",
    " * dst: output image (size dsize & same type as src) \n",
    "    \n",
    "https://docs.opencv.org/4.5.2/da/d6e/tutorial_py_geometric_transformations.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Perspective Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "https://docs.opencv.org/4.5.2/da/d6e/tutorial_py_geometric_transformations.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Thresholding :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T06:46:25.861583Z",
     "start_time": "2021-09-03T06:46:25.838270Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "### Simple Thresholding:\n",
    "* If ***pixel_value > \"threshold\":*** `Assigns 1(may white)` `else Assigns 0(may Black)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@ ret,thre=cv2.threshold(src,thresh,maxval,type)        \n",
    " * scr: Only Gray_scaled Image\n",
    " * thresh: threshold value to consider,used to classify pixel(pixel>thre:1 / 0) \n",
    " * maxval:Value to given, if pixel value is more than(sometimes less than) threshold\n",
    " * type: Type of threshold   \n",
    "    cv2.THRESH_BINARY -\n",
    "    cv2.THRESH_BINARY_INV - \n",
    "    cv2.THRESH_TRUNC - \n",
    "    cv2.THRESH_TOZERO - \n",
    "    cv2.THRESH_TOZERO_INV - \n",
    "\n",
    "    \n",
    "ret,thre=cv2.threshold(img,127,255,cv2.THRESH_BINARY)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Adaptive Thresholding \n",
    " * Calculate threshold for small regions of image\n",
    " * returns 'different thresholds' for 'different regions' of same image \n",
    " * It gives us better results for images with varying illumination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@ ret,thre=cv2.adaptiveThreshold(src,maxValue,adaptiveMethod,thresholdType,blockSize,C)\n",
    " * src : gray_scaled image\n",
    " * maxValue : Value to given, if pixel value is more than(sometimes less than) threshold \n",
    " * blockSize : decides the size of neighbourhood area.\n",
    " * C : (constant)which is subtracted from the mean or weighted mean calculated\n",
    " * adaptiveMethod : decides how thresholding value is calculated\n",
    "     cv2.ADAPTIVE_THRESH_MEAN_C :threshold value is mean of neighbourhood area.\n",
    "     cv2.ADAPTIVE_THRESH_GAUSSIAN_C :threshold value is weighted sum of neighbourhood values where weights are 'gaussian window'\n",
    " * thresholdType : Type of threshold   \n",
    "     cv2.THRESH_BINARY -\n",
    "     cv2.THRESH_BINARY_INV - \n",
    "     cv2.THRESH_TRUNC - \n",
    "     cv2.THRESH_TOZERO - \n",
    "     cv2.THRESH_TOZERO_INV -\n",
    "    \n",
    "\n",
    "thre=cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,11,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otsu’s Binarization\n",
    "* Automatically calculates 'threshold' from image histogram for 'bimodal image' \n",
    " \n",
    "* ***Working:***    \n",
    "    * For Bimodal images,it tries to find 'threshold_value(t)'(minimizes 'weighted within-class variance')\n",
    "* ***Implemention:*** \n",
    "    * Here,'cv2.threshold()' used,but uses extra flag'cv2.THRESH_OTSU' & (for thresh=0) \n",
    "    * So, Algo finds 'optimal threshold_value' & returns 'ret'\n",
    "    * If 'Otsu thresholding' not used,then 'ret' returns 'simple threshold value'\n",
    " Note: For images which are not bimodal,binarization won’t be accurate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ ret,thre=cv2.threshold(src,thresh,maxval,type+cv2.THRESH_OTSU)        \n",
    " * scr: Only Gray_scaled Image\n",
    " * thresh:Used to classify pixel(in this case thres=0) \n",
    " * maxval:Value to given, if pixel value is more than(sometimes less than) threshold\n",
    " * type: Type of threshold   \n",
    "    cv2.THRESH_BINARY -\n",
    "    cv2.THRESH_BINARY_INV - \n",
    "    cv2.THRESH_TRUNC - \n",
    "    cv2.THRESH_TOZERO - \n",
    "    cv2.THRESH_TOZERO_INV - \n",
    " \n",
    "\n",
    "ret,thre=cv2.threshold(img,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing Images :\n",
    " * Images can be 'filtered' with various 'low-pass filters(LPF)',that remove noise/blurr image\n",
    " * It is useful for removing noise \n",
    " * Removes high-frequency content(noise,edges),that results blurred edges when this is filter is applied.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Image Filtering: Using 2D Convolution \n",
    " - Images can be 'filtered' with various 'low-pass filters(LPF)',that remove noise/blurr image\n",
    " - 'cv2.filter2D()' convolves a kernel with an image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T06:58:45.482929Z",
     "start_time": "2021-09-03T06:58:37.179633Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@ filter=filter2D(src,ddepth,kernel)\n",
    " * ddepth: desired depth of ' output image'\n",
    " * kernel: convolution kernel(correlation kernel),a single-channel floating point\n",
    "    \n",
    "kernel=np.ones((5,5),np.float32)/25\n",
    "filters=cv2.filter2D(img,-1,kernel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Image Blurring(Smoothing Image): Using Averging\n",
    " - Done by convolving image with a 'normalized box filter'\n",
    " - Simply takes 'average of all pixels under kernel' & replaces 'central element with this average'\n",
    " - 'cv2.blur() ' / ' cv2.boxFilter() ' can be used for this operation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T07:01:07.678120Z",
     "start_time": "2021-09-03T07:00:32.064653Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### boxFilter() \n",
    "@ filter=cv2.boxFilter(src,ddepth,(ksize),normalize,borderType)\n",
    " * ddepth:  output image depth(-1 to use src.depth())\n",
    " * ksize: blurring kernel size\n",
    " * anchor: anchor point;anchor is at the kernel center . default value (-1,-1)\n",
    " * normalize: specifying whether 'kernel is normalized by its area or nt' (True:enable/False:Disable)\n",
    " * borderType: border mode used to extrapolate pixels outside of the image\n",
    "    \n",
    "filters=cv2.boxFilter(img,ksize=(5,5),normalize=False)  ## using cv2.blur()\n",
    "\n",
    "\n",
    "### cv2.blur()\n",
    "@ filter=cv2.blur(src,(ksize))    \n",
    " * ksize: blurring kernel size\n",
    "\n",
    "filters=cv2.blur(img,(5,5))  ## using cv2.blur()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Gaussian Blurring : 'Gaussian kernel' is used, instead of 'box filter\n",
    " - Highly effective in removing Gaussian noise from image.\n",
    " - Takes width & height of kernel (i.e. +ve & odd value)\n",
    " - specify std in X & Y directions, sigmaX & sigmaY respectively(sigmaX is specified)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@ blur=GaussianBlur(src,ksize,sigmaX,sigmaY,borderType)    \n",
    " * ksize : Gaussian kernel size(ksize.width,ksize.height)    (+ve & odd value)\n",
    " * sigmaX,sigmaY : Gaussian kernel std in X,Y direction(if sigmaY=0,then equals to sigmaX)\n",
    " * borderType : border mode used to extrapolate pixels outside of the image \n",
    "    \n",
    "blur=cv2.GaussianBlur(img,(5,5),0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Median Blurring  : Using cv2.medianBlur()\n",
    "- Computes 'median of all pixels' underkernel window & 'central pixel' is replaced with this 'median value'\n",
    "- Central element is always replaced by some pixel value in image.\n",
    "- Highly effective in removing 'salt-pepper noise'(kernel size must be +ve & odd )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@ blur=cv2.medianBlur(src,ksize)\n",
    " * ksize: aperture linear size(must be odd & +ve & greater than 1,  e.g: 3,5,7)\n",
    "\n",
    "blur=cv2.medianBlur(img,5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Bilateral Filtering : Using cv2.bilateralFilter()\n",
    "- Uses 'Gaussian filter' in space domain,butalso uses one more(multiplicative) Gaussian filter component \n",
    "- 'Gaussian function of space' makes sure that only pixels,'spatial neighbors' are considered for filtering\n",
    "- while 'Gaussian component' ensures that only those pixels with intensities similar to that of central pixel(‘intensity neighbors’) are included to compute blurred intensity value.\n",
    "- Highly effective at 'noise removal' while 'preserving edges'.slower compared to other filters \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@ blur=cv2.bilateralFilter(src,d,sigmaColor,sigmaSpace,borderType)\n",
    " * d: Diameter of each pixel neighborhood that is used during filtering(must be 'non-positive',computed from sigmaSpace)\n",
    " * sigmaColor:Filter sigma in color space.(larger value,tends larger areas of semi-equal color)\n",
    " * sigmaSpace: Filter sigma in coordinate space(larger value, influence each other as long as their colors are close enough)\n",
    " * borderType : border mode used to extrapolate pixels outside of the image\n",
    "    \n",
    "\n",
    "blur=cv2.bilateralFilter(img,9,75,75) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Morphological Transformations :\n",
    " - Normally performed on 'binary images'& Needs two inputs:<br>\n",
    "       1) **original image** <br>\n",
    "       2) **structuring element|kernel(i.e. decides 'nature of operation')**<br> \n",
    " \n",
    "**Two basic morphological operators**\n",
    " 1) ***Erosion***<br>\n",
    " 2) ***Dilation***<br>\n",
    " \n",
    "**Other Variants**: <br>\n",
    " ***Opening*** , ***Closing*** , ***Gradient*** etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i) Erosion: 'erodes away boundaries' of foreground object(Always try to keep foreground in white)\n",
    " - kernel slides through image(as in 2D convolution).\n",
    " - pixel in original image(1|0) will be considered 1: if all pixels under kernel is 1\n",
    "                                                 else it is eroded(made to 0)\n",
    " - 'pixels near boundary' will be 'discarded' depending upon size of kernel.\n",
    " - So thickness|size of foreground object 'decreases|simply white region decreases' in image\n",
    "Used: Removing small white noises\n",
    "@ erosion=erode(src,kernel,anchor,iterations,borderType)\n",
    " * kernel: structuring element used for erosion(created using #getStructuringElement)\n",
    " * anchor: position of anchor within element(default value(-1,-1) means anchor @ element center)\n",
    " * iterations: number of times erosion is applied.\n",
    " * borderType: pixel extrapolation method\n",
    " * borderValue: border value in case of a constant border\n",
    "    \n",
    ">>> kernel=np.ones((5,5),np.uint8)\n",
    ">>> erosion=cv2.erode(img,kernel,iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii) Dilation : Just opposite of erosion.\n",
    " - pixel element is '1': if atleast one pixel under kernel is ‘1’\n",
    " - So it increases 'white region' in image / size of foreground object increases. \n",
    " - In cases of 'noise removal', erosion is followed by dilation.Cuz erosion removes white noises,but it also shrinks our object. \n",
    " - So we dilate it(Since noise is gone, they won’t come back) but our object area increases.\n",
    "Useful: Join broken parts of an object.\n",
    "\n",
    "@ dilation=cv2.dilate(src,kernel,anchor,iterations,borderType,borderValue)\n",
    " * kernel: structuring element used for dilation(created using #getStructuringElement)\n",
    " * anchor: position of anchor within element(default value(-1,-1) means anchor @ element center)\n",
    " * iterations: number of times dilation is applied.\n",
    " * borderType: pixel extrapolation method\n",
    " * borderValue: border value in case of a constant border\n",
    "    \n",
    ">>> kernel=np.ones((5,5),np.uint8)\n",
    ">>> dilation=cv2.dilate(img,kernel,iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iii) Opening : Refers to 'erosion followed by dilation'\n",
    "Used: Removing noise\n",
    "\n",
    "@ opening=cv2.morphologyEx(src,op,kernel,anchor,iterations,borderType,borderValue)\n",
    " * op: Type of a morphological operation(here op=cv2.MORPH_OPEN)\n",
    " * kernel: Structuring element(created using #getStructuringElement)\n",
    " * anchor: Anchor position with kernel (-ve values mean that anchor @ kernel center)\n",
    " * iterations: Number of times erosion & dilation are applied.\n",
    " * borderType: pixel extrapolation method\n",
    " * borderValue: border value in case of a constant border\n",
    " \n",
    ">>> kernel=np.ones((5,5),np.uint8)\n",
    ">>> opening=cv2.morphologyEx(img,cv2.MORPH_OPEN,kernel)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iv) Closing : Using cv2.morphologyEx()\n",
    " - Reverse of Opening,'dilation followed by erosion'\n",
    "Used: Closing small holes inside foreground objects|small black points on object\n",
    "@ closing=cv2.morphologyEx(src,op,kernel,anchor,iterations,borderType,borderValue)\n",
    " * op: Type of a morphological operation(here op=cv2.MORPH_CLOSE)\n",
    " * kernel: Structuring element(created using #getStructuringElement)\n",
    " * anchor: Anchor position with kernel (-ve values mean that anchor @ kernel center)\n",
    " * iterations: Number of times erosion & dilation are applied.\n",
    " * borderType: pixel extrapolation method\n",
    " * borderValue: border value in case of a constant border\n",
    " \n",
    ">>> kernel=np.ones((5,5),np.uint8)\n",
    ">>> closing=cv2.morphologyEx(img,cv2.MORPH_CLOSE,kernel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v) Morphological Gradient: It is difference between 'dilation' & 'erosion' of an image                               \n",
    "                               \n",
    "@ gradient=cv2.morphologyEx(img,cv2.MORPH_GRADIENT,kernel)                               \n",
    " * op: Type of a morphological operation(here op=cv2.MORPH_GRADIENT)\n",
    " * kernel: Structuring element(created using #getStructuringElement)\n",
    " * anchor: Anchor position with kernel (-ve values mean that anchor @ kernel center)\n",
    " * iterations: Number of times erosion & dilation are applied.\n",
    " * borderType: pixel extrapolation method\n",
    " * borderValue: border value in case of a constant border\n",
    " \n",
    ">>> kernel=np.ones((5,5),np.uint8)                                                              \n",
    ">>> gradient=cv2.morphologyEx(img,cv2.MORPH_GRADIENT,kernel) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi) Top Hat : It is difference between 'input image' & 'Opening of image'                               \n",
    "@ tophat=cv2.morphologyEx(img,cv2.MORPH_TOPHAT,kernel)\n",
    " * op: Type of a morphological operation(here op=cv2.MORPH_TOPHAT)\n",
    " * kernel: Structuring element(created using #getStructuringElement)\n",
    " * anchor: Anchor position with kernel (-ve values mean that anchor @ kernel center)\n",
    " * iterations: Number of times erosion & dilation are applied.\n",
    " * borderType: pixel extrapolation method\n",
    " * borderValue: border value in case of a constant border\n",
    "\n",
    ">>> kernel=np.ones((5,5),np.uint8)                              \n",
    ">>> tophat=cv2.morphologyEx(img,cv2.MORPH_TOPHAT,kernel)                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vii) Black Hat : It is difference between 'closing of input image' & 'input image'\n",
    "@ blackhat=cv2.morphologyEx(img,cv2.MORPH_BLACKHAT,kernel)\n",
    " * op: Type of a morphological operation(here op=cv2.MORPH_BLACKHAT)\n",
    " * kernel: Structuring element(created using #getStructuringElement)\n",
    " * anchor: Anchor position with kernel (-ve values mean that anchor @ kernel center)\n",
    " * iterations: Number of times erosion & dilation are applied.\n",
    " * borderType: pixel extrapolation method\n",
    " * borderValue: border value in case of a constant border\n",
    "\n",
    ">>> kernel=np.ones((5,5),np.uint8)                              \n",
    ">>> blackhat=cv2.morphologyEx(img,cv2.MORPH_BLACKHAT,kernel)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "### Structuring Element & Kernel: We can manually create 'structuring elements' help of Numpy(rectangular shape)\n",
    "   - elliptical/circular shaped kernel\n",
    "   -  pass shape & size of kernel,get desired kernel\n",
    "@ kernel=cv2.getStructuringElement(shape,ksize,anchor)\n",
    " * ksize: Size of structuring element(row,col)\n",
    " * anchor: Anchor position within element(default (-1,-1):anchor @ center)\n",
    " * shape: Element shape \n",
    "    - cv2.MORPH_RECT: Rectangle\n",
    "    - cv2.MORPH_ELLIPSE: ellipse\n",
    "    - cv2.MORPH_CROSS :cross - shaped\n",
    "        \n",
    ">>> kernel=cv2.getStructuringElement(cv2.MORPH_RECT,(5,5))  # Rectangular Kernel\n",
    ">>> cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(5,5))      # Elliptical Kernel\n",
    ">>> cv2.getStructuringElement(cv2.MORPH_CROSS,(5,5))        # Cross-shaped Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Gradients: \n",
    " - OpenCV provides three types of 'gradient filters'|'High-pass filters' <br>\n",
    "    ***Sobel X*** , ***Sobel Y*** , ***Laplacian***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i) Sobel & Scharr Derivatives : this Gausssian smoothing plus differentiation operation (more resistant to noise).\n",
    " - We can specify direction of derivatives to be taken(vertical|horizontal using 'yorder' & 'xorder' respectively) \n",
    " - We can specify 'size of kernel' by ksize(If ksize=-1: '3x3 Scharr filter used, gives better results' than '3x3 Sobel filter')\n",
    "@ sobel=Sobel(src,ddepth,dx,dy,ksize,scale,delta,borderType)\n",
    " * ddepth: output image depth\n",
    " * dx,dy: order of derivative x & y\n",
    " * ksize: size of extended Sobel kernel(must be odd)\n",
    " * scale: optional scale factor for computed derivative values(default None)\n",
    " * delta: optional delta value that is added to  results prior to storing them in dst.\n",
    " * borderType: pixel extrapolation method\n",
    "        \n",
    ">>> sobelx=cv2.Sobel(img,cv2.CV_64F,1,0,ksize=5)    ## Sobel X\n",
    ">>> sobely=cv2.Sobel(img,cv2.CV_64F,0,1,ksize=5)    ## Sobel Y   \n",
    "\n",
    "\n",
    "\n",
    "Output Data Type:\n",
    " - 'Black to White transition' taken as '+ve slope'(+ve value)\n",
    " - 'White to Black transition' taken as '-ve slope'(-ve value)\n",
    "So when you convert data to 'np.uint8',then all \" -ve slopes made zero \"(misses the edge)\n",
    "If we want to \"detect both edges\", keep \"output datatype to some higher\"forms(like cv2.CV_16S,cv2.CV_64F etc)\n",
    "take its absolute value & convert back to 'cv2.CV_8U' .\n",
    "\n",
    "\n",
    "sobelx8u=cv2.Sobel(img,cv2.CV_8U,1,0,ksize=5)     # Output dtype = cv2.CV_8U\n",
    "sobelx64f=cv2.Sobel(img,cv2.CV_64F,1,0,ksize=5) # O/p dtype=cv2.CV_64F.Then take its absolute and convert to cv2.CV_8U\n",
    "\n",
    "abs_sobel64f=np.absolute(sobelx64f)\n",
    "sobel_8u=np.uint8(abs_sobel64f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii) Laplacian Derivatives : Calculates Laplacian of the image given by relation.\n",
    " - each derivative is found using 'Sobel derivatives'\n",
    "@ laplacian=cv2.Laplacian(src,ddepth,ksize,scale,delta,borderType)\n",
    " * ddepth: output image depth\n",
    " * ksize: size of extended Sobel kernel(must be odd)\n",
    " * scale: optional scale factor for computed derivative values(default None)\n",
    " * delta: optional delta value that is added to  results prior to storing them in dst.\n",
    " * borderType: pixel extrapolation method\n",
    "     \n",
    ">>> laplacian=cv2.Laplacian(img,cv2.CV_64F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Canny Edge Detection:\n",
    " - Popular 'edge detection algorithm',was developed by 'John F. Canny' in 1986 \n",
    "It's multi-stage algorithm :\n",
    " \n",
    " 1) **Noise Reduction:**\n",
    "     'remove noise' using '5x5 Gaussian filter' from image.\n",
    " \n",
    " 2) **Finding Intensity Gradient of the Image:** \n",
    "    - Smoothened image,filtered with 'Sobel kernel'(both horizontal & vertical direction)\n",
    "    - Which get first derivative in \"horizontal direction(Gx)\" & \"vertical direction(Gy)\" \n",
    "    - From these 2 images,we can find 'Edge_Gradient' & 'direction for each pixel' \n",
    "                 \n",
    "                    Edge_Gradient(G) =sqrt{(Gx)^2 + (Gy)^2}\n",
    "\n",
    "                      Angle(theta) =tan(inv) (Gy/Gx)\n",
    "\n",
    "    - Gradient direction is always 'perpendicular to edges'.So,rounded to one of four angles(representing vertical,horizontal & two diagonal directions)\n",
    " \n",
    " 3) **Non-maximum Suppression** :\n",
    "    -After getting 'Edge_Gradient & direction',a full scan of image is done to 'remove any unwanted pixels',which maynt constitute edge\n",
    "    -At every pixel,'pixel is checked, if it is a local maximum' in its neighborhood in direction of gradient\n",
    "    -If it forms 'local maximum', it is considered for next stage, Else it is suppressed( put to 0)\n",
    "  - In short, result we get is a 'binary image with “thin edges” \n",
    "  \n",
    " 4) **Hysteresis Thresholding** :\n",
    "    This stage decides, which are all edges are really edges & which arent,that need 2 threshold values, 'minVal' & 'maxVal'. \n",
    "    -Edges with 'intensity gradient > maxVal': sure to be edges & 'intensity gradient < maxVal': sure to be non-edges(are discarded)\n",
    "    \n",
    "    -Those who lie between these 2 thresholds are classified 'edges' / 'non-edges' based on their connectivity.\n",
    "    -If they 'connected to “sure-edge” pixels': considered to be part of edges. Else they are discarded\n",
    "    This stage, 'removes small pixels noises' on assumption that edges are long lines.So we finally, get 'strong edges' in image\n",
    "\n",
    "Canny Edge Detection in OpenCV :\n",
    " - OpenCV puts all the above in single function'cv2.Canny()'\n",
    " - First argument is our input image. \n",
    "     Last argument is L2gradient which specifies the equation for finding gradient magnitude. If it is True,\n",
    "        it uses the equation mentioned above which is more accurate,\n",
    "otherwise it uses this function: . By default, it is False.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ edge=cv2.Canny(image,threshold1,threshold2,edges,apertureSize,L2gradient)\n",
    " * image:\n",
    " * threshold1:'minVal threshold'\n",
    " * threshold2:'maxVal threshold'\n",
    " * edges:output edge map, single channels 8-bit image, which has same size as image\n",
    " * apertureSize: 'Size of Sobel kernel' used for find image gradients (default = 3)\n",
    " * L2gradient: Specifies equation for finding Edge_Gradient(G)=|Gx|+|Gy| If True. (Default False) \n",
    "\n",
    ">>> edge=cv2.Canny(img,threshold1=100,threshold2=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img=cv2.imread(r\"C:\\Users\\joker\\Desktop\\opencv-course-master\\Resources\\Photos\\park.jpg\")\n",
    "cv2.imshow('image',img)  # Display image with CV\n",
    "\n",
    "rotation=cv2.getRotationMatrix2D((50,200),90,1)\n",
    "cv2.imshow('R',rotation)\n",
    "#cv2.imshow('2',scale2)\n",
    "#cv2.imshow('3',scale3)\n",
    "\n",
    "cv2.waitKey(0)  # Wait to End:Keyboard binding {0:wait until any key pressed}\n",
    "cv2.destroyAllWindows()            # Destroy all image windows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3|iii) Performance Measurement and Improvement Techniques\n",
    "- In image processing,we are dealing with large no. of operations per second\n",
    "- It is mandatory, providing correct solution in 'fastest manner'\n",
    " * To measure performance of code\n",
    " * Tips to improve performance of code\n",
    "\n",
    "                      a) Performance Measure\n",
    "\n",
    "##### Performance Measuring with OpenCV\n",
    "*'cv2.getTickCount'returns 'no. of clock-cycles after event A to event B'\n",
    "  - we can call it, before(event A) & after(event B) of the function execution(get 'clock-cycles' used to execute function)\n",
    "\n",
    "*'cv2.getTickFrequency' returns 'frequency of clock-cycles / no. of clock-cycles per secs',that find time of execution in secs \n",
    "\n",
    ">>> event_A=cv2.getTickCount() ## before code Execution\n",
    "## your code execution\n",
    ">>> event_B=cv2.getTickCount() ## after code Execution         \n",
    ">>> time=(event_A - event_B)/cv2.getTickFrequency()\n",
    ">>> time \n",
    "\n",
    "\n",
    "##### Performance Measuring with IPython\n",
    "\n",
    "\n",
    "                         b) Optimization \n",
    "\n",
    "##### Optimizationin OpenCV\n",
    "- Many OpenCV functions are optimized using 'SSE2','AVX'.Also contains unoptimized code also.\n",
    "- OpenCV runs optimized code if it is enabled, else it runs the unoptimized code.\n",
    "- We can use 'cv2.useOptimized()' to check if it is enabled/disabled & 'cv2.setUseOptimized()' to enable/disable it.\n",
    "\n",
    ">>> cv2.useOptimized() ## check if optimization is enabled\n",
    ">>> cv2.setUseOptimized(False)  ## Disable it\n",
    "\n",
    "\n",
    "\n",
    "                              c) Performance Optimization Techniques\n",
    "\n",
    "1) Avoid 'loops in Python' as possible,like double/triple loops etc(They are inherently slow)\n",
    "2) 'Vectorize algorithm/code' at max possible extent because Numpy & OpenCV are optimized for vector operations.\n",
    "3) Exploit cache coherence.\n",
    "4) 'Never make copies' of array unless needed(Try to use views instead as 'Array copying is costly operation') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h) \"\"\" Image Pyramids \"\"\"\n",
    "-when, we need to work with images of different resolution of same image\n",
    "-Create set of images with different resolution & search for object in all images,these set of images(different resolution) 'Image Pyramids',cuz they kept biggest image at bottom & smallest image at top \n",
    "Two kinds of Image Pyramids: (1) Gaussian Pyramid  (2) Laplacian Pyramids\n",
    "\n",
    "(1) Gaussian Pyramid:    \n",
    "-'Higher level(Low resolution)' formed by removing consecutive 'rows' & 'columns' in 'Lower level(higher resolution)' image \n",
    "-Then each pixel in 'higher level',formed by contribution from 5 pixels in underlying level with gaussian weights.\n",
    "so,'M*N' image becomes M/2 * N/2 image(area reduces to one-fourth of original area),called 'Octave' \n",
    "     i) 'CV2.pyrDown() :Lower Resolution '\n",
    "       #@ lower=pyrDown(src,dstsize,borderType)\n",
    "        >>> low=cv2.pyrDown(img)\n",
    "        >>> low1=cv2.pyrDown(low)\n",
    "        >>> low2=cv2.pyrDown(low1)\n",
    " \n",
    "     ii) 'CV2.pyrUp() :Higher Resolution '\n",
    "       #@ upper=pyrUp(src,dstsize,borderType)\n",
    "        >>> up=cv2.pyrUp(img)\n",
    "        >>> up1=cv2.pyrUp(up)\n",
    "        >>> up2=cv2.pyrUp(up1)\n",
    "\n",
    "(2) Laplacian Pyramid :\n",
    " -Formed from 'Gaussian Pyramids'.Laplacian pyramid images are like edge images only(Most elements are zeros)\n",
    " -Laplacian Pyramid is formed by difference between that level in Gaussian Pyramid & expanded version of its upper level in Gaussian Pyramid. \n",
    " -Used in image compression.\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i) \"\"\" Contours \"\"\"\n",
    "contour : A curve joining all continuous points(along the boundary), having same color|intensity\n",
    "          Usefull for 'shape analysis','object detection' & 'recognition'\n",
    "   * For better accuracy,'use binary images'.before finding contours,apply threshold/canny edge detection\n",
    "   * 'findContours()' modifies source image.we can store original image to another var\n",
    "   * Finding contours is like finding 'white object' from 'black background'(object to be found: in white & backgroud: in black)\n",
    "\n",
    "a)\n",
    "### Find Contours :\n",
    "  -'Contours' can be find using 'cv2.findContours()'.\n",
    "  - This can take 3 output image,contours(list),hierarchy(nd-array)\n",
    "     where image: Returns 'original image'\n",
    "           contours: Returns 'list of all contours'(Each contour is array(x,y) coordinates of boundary points of object)\n",
    "           hierarchy: Returns ''\n",
    "   Requirement & Steps(for finding contours):\n",
    "    1) convet image to Gray_Scale\n",
    "    2) Threshold \n",
    "    3) cv2.findContours()\n",
    "    \n",
    " #@ contours,hierarchy=cv2.findContours(image,mode,method,contours,hierarchy,offset)\n",
    "   * mode: Contour retrieval mode\n",
    "      -cv2.RETR_TREE:\n",
    "   * method: Contour approximation method\n",
    "      -cv2.CHAIN_APPROX_NONE : all the boundary points are stored\n",
    "      -cv2.CHAIN_APPROX_SIMPLE : \n",
    "   * contours:\n",
    "   * hierarchy:\n",
    "   * offset: Optional offset by which every contour point is shifted\n",
    "\n",
    ">>> img_gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)    ## img to Gray_Scale\n",
    ">>> ret,thresh=cv2.threshold(img_gray,127,255,0)     ## Threshold Of Image\n",
    ">>> contours,hierarchy=cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)  ## Find Contour\n",
    "\n",
    "\n",
    "### Draw Contours :\n",
    " - To draw contours, 'cv2.drawContours()' is used & output contours(list)\n",
    " - It can also be used to draw any shape provided you have its boundary points.\n",
    " #@ (image,contours,contourIdx,color,thickness,lineType,hierarchy,maxLevel,offset)\n",
    "   * image: Destination image.\n",
    "   * contours: All input contours(Each contour stored as point vector)\n",
    "   * contourIdx: Which contour to draw(If -1: draw all contour Else specific contour)\n",
    "   * color: Color of contours(in form of range(like, green=(0,255,255)))\n",
    "   * thickness: Thickness of lines, contours are drawn \n",
    "   * lineType: Line connectivity\n",
    "   * hierarchy: Optional information about hierarchy\n",
    "   * maxLevel: Maximal level for drawn contours\n",
    "   * offset: Optional contour shift parameter. Shift all drawn contours by specified space\n",
    "    \n",
    ">>> img=cv2.drawContours(img,contours,-1,(150,25,255),3)  \n",
    "\n",
    "\n",
    "b) Contour Features:\n",
    " ### Moments :\n",
    "  - It is weighted average of image pixels' intensities\n",
    "  - Usefull for describe area(total intensity) , centroid & its orientation\n",
    "  -'cv2.moments()' gives a dictionary of all moment values calculated\n",
    " #@ \n",
    " >>> m=cv2.moments(contours[0])\n",
    " ### Contour area: computes 'contour area'\n",
    "  -It is given by function 'cv2.contourArea()' / from moments:'M[‘m00’]'\n",
    "  #@ area=cv2.contourArea(contour,oriented)\n",
    "     * contour: contour Input vector of 2D points\n",
    "     * oriented: Oriented area flag(If True,function returns 'signed area value',depending on contour orientation)\n",
    "   >>> area=cv2.contourArea(contours[2])  \n",
    "\n",
    " ### Contour Perimeter :\n",
    "  -It is also called arc length. \n",
    "  -Perform using 'cv2.arcLength()' .\n",
    "    \n",
    "  #@ perimeter=cv2.arcLength(curve,closed)\n",
    "    * curve: Input vector \n",
    "    * closed: Flag indicating whether curve is closed / not\n",
    "   >>> perimeter=cv2.arcLength(contours[1],True)     \n",
    "\n",
    " ### Contour Approximation\n",
    "  - It approximates contour shape to another shape with less no. of vertices depending upon precision we specify\n",
    "  - \n",
    "    \n",
    " \n",
    "\n",
    " ### Convex Hull :\n",
    "   -checks curve for 'convexity defects' & 'corrects it'\n",
    "   convexity defects : \n",
    "  #@ hull= v2.convexHull(points,hull,clockwise,returnPoints]\n",
    "   * points: contours we pass into.\n",
    "   * hull: output, normally we avoid it.\n",
    "   * clockwise : Orientation flag(If True:output convex hull is oriented clockwise)\n",
    "   * returnPoints : If True:returns coordinates of  hull points(If False:returns indices of contour points corresponding to hull points)\n",
    "  >>> hull=cv2.convexHull(Contours)\n",
    "\n",
    " ###  Checking Convexity:\n",
    "   -Process to check, if curve is convex or not\n",
    "   -'cv2.isContourConvex()' return whether True / False\n",
    "  #@ C=cv2.isContourConvex(Contours)\n",
    "    \n",
    "   >>> C=cv2.isContourConvex(Contours[2])\n",
    "\n",
    " ### Bounding with Straight Rectangle :\n",
    " - Bounding contour with straight Rectngle\n",
    " >>> x,y,w,h=cv2.boundingRect(contours[10]) ## bounding specific contour\n",
    " >>> img=cv2.rectangle(img,(x,y),(x+w,y+h),(0,0,0),2) \n",
    " \n",
    " ### Bounding with Rotated Rectangle :\n",
    " - \n",
    " >>> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j) Histograms in OpenCV : \n",
    "\n",
    "histogram: Its a graph,that produces 'intensity distribution' of image. x-axis contains pixel values & y-axis contains corresponding no of pixels \n",
    " - histogram provides, intuition about 'contrast','brightness','intensity distribution' \n",
    "Bins: \n",
    "Dims: It is number of parameters for which we collect data\n",
    "Range:  It is the range of intensity values you want to measure\n",
    "\n",
    "    \n",
    "    i) Calculating & Drawing Histogram \n",
    "\n",
    "## calculating Histogram in OpenCV\n",
    "-\n",
    "#@ cv2.calcHist(images,channels,mask,histSize,ranges,hist,accumulate)\n",
    " * images : image of type uint8 or float32(must be in square brackets like [img])\n",
    " * channels : index of channel for which we calculate histogram(must be in square bracket)\n",
    "    for gray_scale:[0] , for color:[0],[1],[2] \n",
    " * mask : mask image,if we want to calculate histogram in specific region(if whole image mask:None)\n",
    " * histSize: Bin counts(must be in square brackets),for full range pass [256] \n",
    " * ranges :Ranges , normally[0,256]\n",
    ">>> hist=cv2.calcHist([img],[0],None,[256],[0,256])\n",
    "\n",
    "## Plotting Histogram Using Matplotlib\n",
    "\n",
    ">>> from matplotlib import pyplot as plt\n",
    ">>> color = ('b','g','r')\n",
    ">>> for i,col in enumerate(color):\n",
    "        hist=cv2.calcHist([img],[i],None,[256],[0,256])\n",
    "        plt.plot(hist,color=col)\n",
    "        plt.xlim([0,256])\n",
    "    plt.show()\n",
    "    \n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "ii) Histogram Equalization :\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k) \" Template Matching \"\n",
    "Process of searching & finding, location of 'template image' in larger image \n",
    "\n",
    "-'cv2.matchTemplate()':It simply slides template over image(as in 2D convolution) & compares template & patch image under template image.\n",
    "- Returns a grayscale image, where each pixel denotes how much does the neighbourhood of that pixel match with template.\n",
    "#@ tm=cv2.matchTemplate(image,templ,method,result,mask)\n",
    " * image: larger image(must be 8-bit / 32-bit)\n",
    " * templ: template image(mustnt > source_img & same data type)\n",
    " * result: Map of comparison results(o/p)\n",
    " * method: type of template maching mode\n",
    "    {cv2.TM_CCOEFF,cv2.TM_CCOEFF_NORMED,cv2.TM_CCORR,cv2.TM_CCORR_NORMED,cv2.TM_SQDIFF,cv2.TM_SQDIFF_NORMED}\n",
    " * mask: Mask of searched template. It must have the same datatype and size with templ\n",
    "    \n",
    "-If input image is of size(WxH) & template image is of size(wxh),output image will have size(W-w+1, H-h+1)\n",
    "-After getting templatematch('tm'),we can use 'cv2.minMaxLoc()' to find where is 'max_value & min_value'. \n",
    "#@ min_val,max_val,min_loc,max_loc=cv2.minMaxLoc(src,mask)\n",
    " * src: input single-channel array.\n",
    " * mask: optional mask used to select a sub-array\n",
    " * minVal: pointer to the returned minimum value; NULL is used if not required.\n",
    " * maxVal: pointer to the returned maximum value; NULL is used if not required.\n",
    " * minLoc: pointer to the returned minimum location (in 2D case); NULL is used if not required.\n",
    " * maxLoc: pointer to the returned maximum location (in 2D case); NULL is used if not required.\n",
    "\n",
    "-By Takeing 'max_value & min_value' as 'top-left corner of rectangle' & take template(w,h) as width & height of rectangle.\n",
    "-That is helpful for drawing rectangle in region of template.\n",
    "\n",
    "##### matching Single template \n",
    "import cv2\n",
    "import numpy as np\n",
    "img=cv2.imread(r\"A:\\Python_projects\\tianic_kaggle\\images\\titanic-disaster.jpg\")\n",
    "img_gr=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "temp=cv2.imread(r\"C:\\Users\\Sud()\\Desktop\\OCV_fle\\temp.jpg\",0)\n",
    "\n",
    "w,h=temp.shape[::-1]   ## provide (w,h) of template\n",
    "tm=cv2.matchTemplate(img_gr,temp,cv2.TM_CCOEFF_NORMED)   ## template matching\n",
    "\n",
    "min_val,max_val,min_loc,max_loc=cv2.minMaxLoc(tm)  ## \n",
    "\n",
    "# pt1=min_loc    ## for method:{cv2.TM_SQDIFF,cv2.TM_SQDIFF_NORMED} (pt1 for top_left points)\n",
    "pt1=max_loc      ## for remaining all  (pt1 for top_left points)\n",
    "pt2=(pt1[0]+w, pt1[1]+h)  ## (pt2 for bottom_right points)\n",
    "\n",
    "cv2.rectangle(img,pt1,pt2,(0,0,255),1)  ## setting up rectangle\n",
    "\n",
    "cv2.imshow('detected',img)\n",
    "#cv2.imshow('template',temp)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows() \n",
    "\n",
    "\n",
    "##### Matching multiple Templates\n",
    "import cv2\n",
    "import numpy as np\n",
    "img=cv2.imread(r\"A:\\Python_projects\\tianic_kaggle\\images\\titanic-disaster.jpg\")\n",
    "img_gr=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "temp=cv2.imread(r\"C:\\Users\\Sud()\\Desktop\\OCV_fle\\temp.jpg\",0)\n",
    "\n",
    "w,h=temp.shape[::-1]   ## provide (w,h) of template\n",
    "tm=cv2.matchTemplate(img_gr,temp,cv2.TM_CCOEFF_NORMED)   ## template matching\n",
    "\n",
    "threshold=0.95       ## setting threshold value\n",
    "loc=np.where(tm >= threshold)  ## greater than threshold value\n",
    "\n",
    "for pt1 in zip(*loc[::-1]):     ## finding locations of 'templates' in image (location (w,h))\n",
    "    cv2.rectangle(img,pt1,(pt1[0]+w,pt1[1]+h),(0,0,0),1) ##(W-w+1, H-h+1)\n",
    "\n",
    "cv2.imshow('detected',img)  ## matched image(template patched over image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l) Hough Transform\n",
    "\n",
    "\n",
    "a) Hough Line Transformation\n",
    "\n",
    "\n",
    "\n",
    "b) Hough Line Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img=cv2.imread(r\"C:\\Users\\Sud()\\Desktop\\OCV_fle\\lines.PNG\")\n",
    "#img_gray=cv2.imread(r\"C:\\Users\\Sud()\\Desktop\\OCV_fle\\lines.PNG\",0)\n",
    "cv2.namedWindow('e1',cv2.WINDOW_NORMAL)\n",
    "#cv2.namedWindow('e2',cv2.WINDOW_NORMAL)\n",
    "\n",
    "e1 = cv2.Canny(img,50,150,apertureSize = 3)\n",
    "#e2 = cv2.Canny(img_gray,50,150,apertureSize = 3)\n",
    "\n",
    "lines=cv2.HoughLines(e1,1,np.pi/180,50)\n",
    "for rho,theta in lines[0]:\n",
    "    a = np.cos(theta)\n",
    "    b = np.sin(theta)\n",
    "    x0 = a*rho\n",
    "    y0 = b*rho\n",
    "    x1 = int(x0 + 1000*(-b))\n",
    "    y1 = int(y0 + 1000*(a))\n",
    "    x2 = int(x0 - 1000*(-b))\n",
    "    y2 = int(y0 - 1000*(a))\n",
    "\n",
    "    cv2.line(img,(x1,y1),(x2,y2),(0,0,255),2)\n",
    "\n",
    "\n",
    "\n",
    "#cv2.imshow('e2',e2)\n",
    "cv2.imshow('detected',img)\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines=cv2.HoughLines(image,rho,theta,threshold,lines,srn,stn,min_theta,max_theta)\n",
    "*image : 8-bi (single-channel binary)\n",
    "* rho : \n",
    "* theta \n",
    "* threshold \n",
    "* lines \n",
    "* srn \n",
    "* stn \n",
    "* min_theta \n",
    "* max_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Feature Detection & Description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Machine Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  7) Computational Photography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Camera Calibration & 3D Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Video Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  OpenCV-Python Bindings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flipping\n",
    "flip = cv.flip(img, -1)\n",
    "\n",
    "# Cropping\n",
    "cropped = img[200:400, 300:400]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
