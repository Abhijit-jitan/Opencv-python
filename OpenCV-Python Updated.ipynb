{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T19:58:29.240469Z",
     "start_time": "2021-09-01T19:58:29.174474Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.5.3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gui Features: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started with Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T19:58:32.015613Z",
     "start_time": "2021-09-01T19:58:31.953234Z"
    }
   },
   "outputs": [],
   "source": [
    "## Read Image :\n",
    "img=cv2.imread(r\"C:\\Users\\JokeRR\\Referances\\python_projects\\Number-plate Detection\\image3.jpg\",cv2.IMREAD_COLOR) \n",
    "\n",
    "# Color channels \n",
    "cv2.IMREAD_COLOR|(1) : Colored\n",
    "cv2.IMREAD_GRAYSCALE|(0) : Gray-scale\n",
    "cv2.IMREAD_UNCHANGED|(-1) : Alpha channel\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T19:31:06.705544Z",
     "start_time": "2021-09-01T19:31:00.565710Z"
    }
   },
   "outputs": [],
   "source": [
    "## Window Size:\n",
    "cv2.namedWindow('window_name',cv2.WINDOW_NORMAL)   # Resizable window\n",
    "cv2.namedWindow('window_name',cv2.WINDOW_AUTOSIZE) # Auto-Sized window\n",
    "\n",
    "\n",
    "## Display Image: Using cv2.imshow()\n",
    "cv2.imshow('image_name2',img)  # Show image  @imshow('window_name',image)\n",
    "cv2.waitKey(0)                 # Wait to End:Keyboard binding @cv2.waitKey(any key)\n",
    "cv2.destroyAllWindows()        # Destroy all image windows\n",
    "\n",
    "## Using Matplotlib :\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img,cmap='gray',interpolation='bicubic')# show Image\n",
    "plt.xticks([]),plt.yticks([])                      # hide tick values on X & Y\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write Image:\n",
    "cv2.imread(r'C:\\Users\\joker',1)   #@ cv2.imread(path,color_mode) \n",
    "\n",
    "\n",
    "# Save when button pressed:\n",
    "cv2.imshow(\"Display window\",img)\n",
    "k=cv2.waitKey(0)\n",
    "if k==ord(\"s\"):\n",
    "    cv2.imwrite(\"starry_night.png\",img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic image properties\n",
    "img.size    # Size of image(Total Pixel Size)\n",
    "img.shape   # Shape of image\n",
    "img.dtype   # data type of image\n",
    "img[11,433] # Access Image Pixel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started with Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T20:34:53.773871Z",
     "start_time": "2021-09-01T20:34:53.763829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function putText:\n",
      "\n",
      "putText(...)\n",
      "    putText(img, text, org, fontFace, fontScale, color[, thickness[, lineType[, bottomLeftOrigin]]]) -> img\n",
      "    .   @brief Draws a text string.\n",
      "    .   \n",
      "    .   The function cv::putText renders the specified text string in the image. Symbols that cannot be rendered\n",
      "    .   using the specified font are replaced by question marks. See #getTextSize for a text rendering code\n",
      "    .   example.\n",
      "    .   \n",
      "    .   @param img Image.\n",
      "    .   @param text Text string to be drawn.\n",
      "    .   @param org Bottom-left corner of the text string in the image.\n",
      "    .   @param fontFace Font type, see #HersheyFonts.\n",
      "    .   @param fontScale Font scale factor that is multiplied by the font-specific base size.\n",
      "    .   @param color Text color.\n",
      "    .   @param thickness Thickness of the lines used to draw a text.\n",
      "    .   @param lineType Line type. See #LineTypes\n",
      "    .   @param bottomLeftOrigin When true, the image data origin is at the bottom-left corner. Otherwise,\n",
      "    .   it is at the top-left corner.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cv2.putText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drawing Shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Line \"\"\" \n",
    "Draws Line between pt1 & pt2\n",
    "@ line(img,pt1,pt2,color,thickness,lineType,shift)\n",
    " * img: Image.\n",
    " * pt1: 1st point of line ; pt2: 2nd point of line\n",
    " * color: Line color\n",
    " * thickness: Line thickness.\n",
    " * lineType: Type of the line\n",
    " * shift: Number of fractional bits in point coordinates.\n",
    "\n",
    "cv2.line(img,(100,250),(300,400),(255,0,255),3) \n",
    "\n",
    "\n",
    "\"\"\" Polygon \"\"\"\n",
    "Draws one or more polygonal curves.\n",
    "@ polylines(img,pts,isClosed,color,thickness,lineType,shift)\n",
    " * pts: Array of polygonal curves.\n",
    " * isClosed: whether drawn polylines are closed or not\n",
    " * shift: Number of fractional bits in vertex coordinates\n",
    "\n",
    "coordinates=np.array([[10,5],[20,30],[70,20],[50,10]],np.int32).reshape((-1,1,2))  ## set vertices\n",
    "cv2.polylines(img,[coordinates],True,(0,255,255))\n",
    "\n",
    "\n",
    "\"\"\" Circle \"\"\"\n",
    "Draws simple|filled circle with given center & radius\n",
    "@ circle(img, center, radius, color[, thickness[, lineType[, shift]]]) -> img\n",
    " * center: Center of the circle.\n",
    " * radius: Radius of the circle.\n",
    " * shift: Number of fractional bits in coordinates of center & in radius value.\n",
    "\n",
    "cv.circle(img,(415,215),20,(0,0,255),1)\n",
    "\n",
    "\n",
    "\"\"\" Ellipse \"\"\"\n",
    "Draws simple|thick elliptic arc|fills ellipse sector\n",
    "@ ellipse(img,center,axes,angle,startAngle,endAngle,color,thickness,lin * \n",
    " * center: Center of ellipse\n",
    " * axes: Half of size of ellipse main axes\n",
    " * angle: Ellipse rotation angle in degrees\n",
    " * startAngle: Starting angle of elliptic arc in degrees.\n",
    " * endAngle: Ending angle of elliptic arc in degrees.\n",
    " * shift: Number of fractional bits in coordinates of center & values of axes.\n",
    "    \n",
    "cv2.ellipse(img,(256,256),(100,50),0,0,180,(0,0,255),-1)\n",
    "\n",
    "\n",
    "\"\"\" Rectangle \"\"\"\n",
    "draws outline|filled rectangle whose two opposite corners are pt1 & pt2\n",
    "@ rectangle(img,pt1,pt2,color,thickness,lineType,shift)\n",
    " * pt1: Vertex of rectangle ; pt2: Vertex of rectangle opposite to pt1 \n",
    " * shift: Number of fractional bits in the point coordinates.\n",
    "    \n",
    "cv.rectangle(img,(0,0),(img.shape[1]//2,img.shape[0]//2),(0,255,0),-1)\n",
    "\n",
    "\n",
    "\"\"\" Text \"\"\"\n",
    "Draws a text string\n",
    "@ putText(img,text,org,fontFace,fontScale,color,thickness,lineType,bottomLeftOrigin)\n",
    " * text: Text string to be drawn.\n",
    " * org: Bottom-left corner of text string in image.\n",
    " * fontFace: Font type\n",
    " * fontScale: Font scale factor (multiplied by font-specific base size)\n",
    " * bottomLeftOrigin: True: image data origin is at bottom-left corner; False: top-left corner\n",
    "\n",
    "text=\"LoL !!!!!! \"\n",
    "cv.putText(img,text,(0,225),cv.FONT_HERSHEY_TRIPLEX,1.0,(0,255,0),2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Core Operations :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3|i) \" Basic Operations on images \"\n",
    "#### a) Access image properties\n",
    "\n",
    "* 'size': Returns total pixel size of image\n",
    "\n",
    "* 'shape':Returns shape of image\n",
    "    - for Color image : Returns (Row,Column,Channel)\n",
    "    - for Gray image : Returns (Row,Column) \n",
    "\n",
    "* 'dtype' : Returns data type of image(e.g:unit8) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### b) Access pixel values and modify : for Color images \n",
    "* 'Access Pixel Values' : It can be accessed by its 'row' & 'column'\n",
    "  - For 'BGR image': Returns an array of [Blue_value,Green_value,Red_value]\n",
    "  \n",
    "* 'Access Color values':We can access specific color value(Blue,Green,Red)\n",
    " channel={0:Blue|1:green|2:Red} \n",
    ">>> img[pixel_row,pixel_column,chanel]  \n",
    "or\n",
    ">>>img.item(pixel_row,pixel_column,chanel)\n",
    "\n",
    "* Modify Pixel Value : We can modify pixel value to new value\n",
    "\n",
    ">>> img[pixel_row,pixel_column]=[Blue_value,Green_value,Red_value] ## Modify all values at once\n",
    "or\n",
    ">>> img.itemset((pixel_row,pixel_column,chanel),new value)## can modify specific color value \n",
    "\n",
    "\n",
    "\n",
    "## Access pixel value  \n",
    "pixel=img[pixel_row,pixel_column]\n",
    "or\n",
    "\n",
    "## Access Specific Colored Pixel\n",
    "b=img[pixel_row,pixel_column,0]  # Gets \" Blue Pixel\"  (channel=0)\n",
    "g=img[pixel_row,pixel_column,1]  # Gets \" Green Pixel\" (channel=1) \n",
    "r=img[pixel_row,pixel_column,2]  # Gets \" Red Pixel\"   (channel=2)\n",
    "or\n",
    "blue_pixel=img.item(pixel_row,pixel_column,0) # Gets \" Blue Pixel\"  (channel=0 )\n",
    "green_pixel=img.item(pixel_row,pixel_column,1)# Gets \" Green Pixel\" (channel=1) \n",
    "red_pixel=img.item(pixel_row,pixel_column,2)  # Gets \" Red Pixel\"   (channel=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### c)  Acess pixel value & Modify Values : For Gray Image \n",
    "* 'Access Pixel Values' : It can be accessed by its 'row' & 'column'\n",
    "  - For 'Gray_scaled image' : Returns an array of  \n",
    "\n",
    "* 'Access Color values': We can access specific color value(Blue,Green,Red)\n",
    "\n",
    "* Modify Pixel Value : We can modify pixel value to new value\n",
    " \n",
    "\n",
    "## Modify Pixel Value\n",
    "img[pixel_row,pixel_column]=[Blue_value,Green_value,Red_value] ## Modify all values at once\n",
    "or\n",
    "b=img.itemset((pixel_row,pixel_column,0),new_b value)## access only \" Blue Pixel\"(channel=0 )\n",
    "g=img.itemset((pixel_row,pixel_column,1),new_g value)## access only \" Green Pixel\"(channel=1) \n",
    "r=img.itemset((pixel_row,pixel_column,2),new_r value)## access only \" Red Pixel\"(channel=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-46ab63c41633>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-46ab63c41633>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    b) \"\"\" Access pixel value & Modify Values : For Color Image \"\"\"\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## Set Region of Image (ROI) \n",
    " - ROI:Refers to choosing specific Region of Image,that obtained using 'Numpy Indexing'\n",
    " \n",
    "@ region=img[start_row:end_row,start_col:end_col]  # Selecting Region\n",
    "region=img[400:600,50:200]\n",
    "\n",
    "@ img[start_row:end_row,start_col:end_col]=region  # Pasting Region to another location\n",
    "img[:240,100:260] = region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting & Merging image Channels\n",
    " - cv2.split() & cv2.merge() are very time taking(we can use alternative)\n",
    "\n",
    "\n",
    "## Split Color-channels\n",
    "B,G,R=cv2.split(img)\n",
    "cv2.imshow('Blue',B)\n",
    "cv2.imshow('Green',G)\n",
    "cv2.imshow('Red',R)\n",
    "\n",
    "## Merge Channels:\n",
    "merged=cv2.merge([B,G,R])\n",
    "cv2.imshow('Merged Image',merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Image Border:\n",
    " - We can add border of image(like photo frame) , zero padding etc.\n",
    "    \n",
    "#@ cv2.copyMakeBorder(src,top,bottom,left,right,borderType,value)\n",
    "src:Input image(where to draw border)\n",
    "top,bottom,left,right: border width(in number of pixel)\n",
    "borderType: Border type\n",
    " *cv2.BORDER_CONSTANT - Constant colored border(value=color)\n",
    " *cv2.BORDER_REFLECT - Mirror reflection of border elements (fedcba|abcdefgh|hgfedcb)\n",
    " *cv2.BORDER_REFLECT_101 | cv2.BORDER_DEFAULT - Same as above, but with a slight change, like this : gfedcb|abcdefgh|gfedcba\n",
    " *cv2.BORDER_REPLICATE - Last element is replicated throughout, like this: aaaaaa|abcdefgh|hhhhhhh\n",
    " *cv2.BORDER_WRAP -it will look like this (cdefgh|abcdefgh|abcdefg)\n",
    "value:color value for cv2.BORDER_CONSTANT ([255,0,0]=B|[0,255,0]=G|[0,0,255]=R)\n",
    "\n",
    ">>> border=cv2.copyMakeBorder(img,10,10,10,10,cv2.BORDER_CONSTANT,value=[255,0,0])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Arithmetic Operations on Images\n",
    "               \n",
    "i) Image Addition :Adds multiple images or scalar value  by 'cv2.add()'\n",
    " Note: Imges must be of 'same size','same type' & 'same depth'\n",
    "\n",
    "@ var=cv2.add(img1,img2)\n",
    "    \n",
    "ii) Image Blending : Image addition but 'different weights'can be given,that controlls blending(transparency) \n",
    " - Calculates weighted sum of two images \n",
    "Formula: added=ima1.alpha+img2.beta+gamma\n",
    "\n",
    "@ cv2.addWeighted(src1,alpha,src2,beta,gamma) \n",
    " * alpha: weight for 1st image, perform a cool transition (range from 0-1)\n",
    " * beta: weight for 2st image, perform a cool transition (range from 0-1) \n",
    " * gamma: weight/scalar added to blend,controls Exposure \n",
    "    \n",
    ">>> dst=cv2.addWeighted(img,0.4,img1,0.6,0)\n",
    "                       \n",
    "iii) Bitwise Operations: AND, OR, NOT & XOR operations   \n",
    " - Useful for 'extracting any part of image','defining' & 'working with non-rectangular ROI'\n",
    "\n",
    "rectangle=cv.rectangle(blank.copy(),(30,30),(370,370),255,-1)\n",
    "circle=cv.circle(blank.copy(),(200,200),200,255,-1)\n",
    "\n",
    "# bitwise AND --> intersecting regions\n",
    "bitwise_and=cv.bitwise_and(rectangle,circle)\n",
    "\n",
    "# bitwise OR --> non-intersecting and intersecting regions\n",
    "bitwise_or=cv.bitwise_or(rectangle,circle)\n",
    "\n",
    "# bitwise XOR --> non-intersecting regions\n",
    "bitwise_xor=cv.bitwise_xor(rectangle,circle)\n",
    "\n",
    "# bitwise NOT\n",
    "bitwise_not=cv.bitwise_not(circle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Changing Colorspaces :\n",
    " - Converts images from one color-space to another(BGR -> Gray, BGR -> HSV etc.)\n",
    " - OpenCV supports 150 color-space conversion methods available in \n",
    "\n",
    "# Color  Spaces & Ranges :\n",
    " In Color Images(BGR) -> [Blue,Green,Red] (all ranges from [0,255])\n",
    " In HSV -> [Hue,Saturation,Value] (h range [0,179] & s,v range[0,255])\n",
    " In Gray -> [ ]\n",
    "\n",
    "# List of all 'Color-Conversion Flags':\n",
    ">>> flags=[i for i in dir(cv2) if i.startswith('COLOR_')]\n",
    "\n",
    "# Changing color space : \n",
    "convert images from one color space to another(BGR -> Gray|BGR -> HSV)\n",
    "@ cv2.cvtColor(image_name,flag)    ## flag= Color-Conversion flag\n",
    "\n",
    "gray=cv.cvtColor(img,cv.COLOR_BGR2GRAY)   # BGR to Grayscale\n",
    "hsv=cv.cvtColor(img,cv.COLOR_BGR2HSV)     # BGR to HSV\n",
    "lab=cv.cvtColor(img,cv.COLOR_BGR2LAB)     # BGR to L*a*b\n",
    "rgb=cv.cvtColor(img,cv.COLOR_BGR2RGB)     # BGR to RGB\n",
    "lab_bgr=cv.cvtColor(lab,cv.COLOR_LAB2BGR) # HSV to BGR\n",
    "# can't  convert gray-scale to HSV directly: we have to convert gray->BGR->HSV \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate(img, angle, rotPoint=None):\n",
    "    (height,width) = img.shape[:2]\n",
    "    if rotPoint is None:\n",
    "        rotPoint = (width//2,height//2)\n",
    "    rotMat = cv.getRotationMatrix2D(rotPoint, angle, 1.0)\n",
    "    dimensions = (width,height)\n",
    "    return cv.warpAffine(img, rotMat, dimensions)\n",
    "\n",
    "rotated = rotate(img, -45)\n",
    "rotated_rotated = rotate(img, -225)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Geometric Transformations of Images\n",
    "i) Scaling: Used for 'Resizing image'\n",
    " - Size of image can be specified manually or scaling factor\n",
    "@ cv2.resize(img,,fx,fy,interpolation)\n",
    " *fx,fy :scale along x & y direction\n",
    " *interpolation: type of scaling\n",
    "     cv2.INTER_AREA: Shrink \n",
    "     cv2.INTER_CUBIC: Slow\n",
    "     cv2.INTER_LINEAR: Zooming\n",
    "\n",
    ">>> scale=cv2.resize(img,None,fx=2,fy=2,interpolation=cv2.INTER_CUBIC)    \n",
    "\n",
    "   \n",
    "ii) Translation : Shifting object Location\n",
    "  - Takes a 2x3 transformation matrix as input\n",
    "    \n",
    "    @ shift=cv2.warpAffine(img,np.float32([[1,0,tx],[0,1,ty]]),(cols,rows))\n",
    "\n",
    "   - cv2.warpPerspective - Takes a 3x3 transformation matrix as input\n",
    "    @\n",
    "\n",
    "    \n",
    "\n",
    "##### iii) Rotation  :\n",
    " - \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "##### iv) Translation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translation : shift image\n",
    "def translate(img, x, y):\n",
    "    transMat = np.float32([[1,0,x],[0,1,y]])\n",
    "    dimensions = (img.shape[1], img.shape[0])\n",
    "    return cv.warpAffine(img, transMat, dimensions)\n",
    "\n",
    "# -x --> Left  # -y --> Up\n",
    "# x --> Right  # y --> Down\n",
    "translated = translate(img, -100, 100)\n",
    "\n",
    "\n",
    "# Rotation\n",
    "def rotate(img, angle, rotPoint=None):\n",
    "    (height,width) = img.shape[:2]\n",
    "    if rotPoint is None:\n",
    "        rotPoint = (width//2,height//2)\n",
    "    rotMat = cv.getRotationMatrix2D(rotPoint, angle, 1.0)\n",
    "    dimensions = (width,height)\n",
    "    return cv.warpAffine(img, rotMat, dimensions)\n",
    "\n",
    "rotated = rotate(img, -45)\n",
    "rotated_rotated = rotate(img, -225)\n",
    "\n",
    "# Resizing\n",
    "resized = cv.resize(img, (500,500), interpolation=cv.INTER_CUBIC)\n",
    "\n",
    "# Flipping\n",
    "flip = cv.flip(img, -1)\n",
    "\n",
    "# Cropping\n",
    "cropped = img[200:400, 300:400]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Image Thresholding :\n",
    "\n",
    "i) Simple Thresholding: If pixel_value > than \"threshold\": Assigns 1(may white)\n",
    "        else Assigns 0(may Black)\n",
    "        \n",
    "@ ret,thre=cv2.threshold(src,thresh,maxval,type)        \n",
    " * scr: Only Gray_scaled Image\n",
    " * thresh: threshold value to consider,used to classify pixel(pixel>thre:1 / 0) \n",
    " * maxval:Value to given, if pixel value is more than(sometimes less than) threshold\n",
    " * type: Type of threshold   \n",
    "    cv2.THRESH_BINARY -\n",
    "    cv2.THRESH_BINARY_INV - \n",
    "    cv2.THRESH_TRUNC - \n",
    "    cv2.THRESH_TOZERO - \n",
    "    cv2.THRESH_TOZERO_INV - \n",
    "\n",
    ">>> ret,thresh1=cv2.threshold(img,127,255,cv2.THRESH_BINARY)    \n",
    "       \n",
    "ii) Adaptive Thresholding : Calculate threshold for a small regions of image\n",
    " - So we get 'different thresholds' for 'different regions' of same image \n",
    " - It gives us better results for images with varying illumination\n",
    "@ ret,thre=cv2.adaptiveThreshold(src,maxValue,adaptiveMethod,thresholdType,blockSize,C)\n",
    " * src : gray_scaled image\n",
    " * maxValue : Value to given, if pixel value is more than(sometimes less than) threshold \n",
    " * blockSize : decides the size of neighbourhood area.\n",
    " * C : (constant)which is subtracted from the mean or weighted mean calculated\n",
    " * adaptiveMethod : decides how thresholding value is calculated\n",
    "     cv2.ADAPTIVE_THRESH_MEAN_C :threshold value is mean of neighbourhood area.\n",
    "     cv2.ADAPTIVE_THRESH_GAUSSIAN_C :threshold value is weighted sum of neighbourhood values where weights are 'gaussian window'\n",
    " * thresholdType : Type of threshold   \n",
    "     cv2.THRESH_BINARY -\n",
    "     cv2.THRESH_BINARY_INV - \n",
    "     cv2.THRESH_TRUNC - \n",
    "     cv2.THRESH_TOZERO - \n",
    "     cv2.THRESH_TOZERO_INV -\n",
    "    \n",
    ">>> thre=cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,11,2)\n",
    "\n",
    "\n",
    "iii) Otsu’s Binarization: Automatically calculates 'threshold' from image histogram for 'bimodal image' \n",
    " Working:    \n",
    "  - For Bimodal images,it tries to find 'threshold_value(t)'(minimizes 'weighted within-class variance')\n",
    " Implemention: \n",
    "  - Here,'cv2.threshold()' used,but uses extra flag'cv2.THRESH_OTSU' & (for thresh=0) \n",
    "  - So, Algo finds 'optimal threshold_value' & returns 'ret'\n",
    "  - If 'Otsu thresholding' not used,then 'ret' returns 'simple threshold value'\n",
    " Note: For images which are not bimodal,binarization won’t be accurate\n",
    "\n",
    "@ ret,thre=cv2.threshold(src,thresh,maxval,type+cv2.THRESH_OTSU)        \n",
    " * scr: Only Gray_scaled Image\n",
    " * thresh:Used to classify pixel(in this case thres=0) \n",
    " * maxval:Value to given, if pixel value is more than(sometimes less than) threshold\n",
    " * type: Type of threshold   \n",
    "    cv2.THRESH_BINARY -\n",
    "    cv2.THRESH_BINARY_INV - \n",
    "    cv2.THRESH_TRUNC - \n",
    "    cv2.THRESH_TOZERO - \n",
    "    cv2.THRESH_TOZERO_INV - \n",
    " \n",
    ">>> ret,thre=cv2.threshold(img,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filtering | Smoothing Images\n",
    " - Images can be 'filtered' with various 'low-pass filters(LPF)',that remove noise/blurr image\n",
    " - It is useful for removing noise \n",
    " - Removes high-frequency content(noise,edges),that results blurred edges when this is filter is applied.\n",
    "\n",
    "i) Image Filtering: Using 2D Convolution \n",
    " - Images can be 'filtered' with various 'low-pass filters(LPF)',that remove noise/blurr image\n",
    " - 'cv2.filter2D()' convolves a kernel with an image\n",
    "@ filter=filter2D(src,ddepth,kernel)\n",
    " * ddepth: desired depth of ' output image'\n",
    " * kernel: convolution kernel(correlation kernel),a single-channel floating point\n",
    "    \n",
    ">>> kernel=np.ones((5,5),np.float32)/25\n",
    ">>> filter=cv2.filter2D(img,-1,kernel)\n",
    "\n",
    "\n",
    "ii) Image Blurring(Smoothing Image): Using Averging\n",
    " - Done by convolving image with a 'normalized box filter'\n",
    " - Simply takes 'average of all pixels under kernel' & replaces 'central element with this average'\n",
    " - 'cv2.blur() ' / ' cv2.boxFilter() ' can be used for this operation \n",
    "@ filter=cv2.boxFilter(src,ddepth,(ksize),normalize,borderType)\n",
    " * ddepth:  output image depth(-1 to use src.depth())\n",
    " * ksize: blurring kernel size\n",
    " * anchor: anchor point;anchor is at the kernel center . default value (-1,-1)\n",
    " * normalize: specifying whether 'kernel is normalized by its area or nt' (True:enable/False:Disable)\n",
    " * borderType: border mode used to extrapolate pixels outside of the image\n",
    "or \n",
    "@ filter=cv2.blur(src,(ksize))    \n",
    " * ksize: blurring kernel size\n",
    "\n",
    ">>> filter=cv2.blur(img,(5,5))  ## using cv2.blur()\n",
    ">>> filter=cv2.boxFilter(img,ksize=(5,5),normalize=False)  ## using cv2.blur()\n",
    "\n",
    "\n",
    "iii) Gaussian Filtering :'Gaussian kernel' is used,instead of 'box filter\n",
    " - Highly effective in removing Gaussian noise from image.\n",
    " - Takes width & height of kernel (i.e. +ve & odd value)\n",
    " - specify std in X & Y directions, sigmaX & sigmaY respectively(sigmaX is specified)\n",
    "\n",
    "@ blur=GaussianBlur(ksize,sigma,ktype)\n",
    " * ksize : Aperture size (+ve & odd value)\n",
    " * sigma : Gaussian standard deviation (if non+ve, then calculated as  'sigma = 0.3*((ksize-1)*0.5 - 1') + 0.8)\n",
    " * ktype: Type of filter coefficients{CV_32F or CV_64F} \n",
    "\n",
    "@ blur=GaussianBlur(src,ksize,sigmaX,sigmaY,borderType)    \n",
    " * ksize : Gaussian kernel size(ksize.width,ksize.height)    (+ve & odd value)\n",
    " * sigmaX,sigmaY : Gaussian kernel std in X,Y direction(if sigmaY=0,then equals to sigmaX)\n",
    " * borderType : border mode used to extrapolate pixels outside of the image \n",
    "    \n",
    ">>> blur=cv2.GaussianBlur(img,(5,5),0)\n",
    "\n",
    "\n",
    "\n",
    "##### iv) Median Filtering : Using cv2.medianBlur()\n",
    "- Computes 'median of all pixels' underkernel window & 'central pixel' is replaced with this 'median value'\n",
    "- Central element is always replaced by some pixel value in image.\n",
    "- Highly effective in removing 'salt-pepper noise'(kernel size must be +ve & odd )\n",
    "\n",
    "#@ blur=cv2.medianBlur(src,ksize)\n",
    " * ksize: aperture linear size(must be odd & +ve & greater than 1,  e.g: 3,5,7)\n",
    "\n",
    ">>> blur=cv2.medianBlur(img,5)\n",
    "\n",
    "\n",
    "##### v) Bilateral Filtering : Using cv2.bilateralFilter()\n",
    "- Uses 'Gaussian filter' in space domain,butalso uses one more(multiplicative) Gaussian filter component \n",
    "- 'Gaussian function of space' makes sure that only pixels,'spatial neighbors' are considered for filtering\n",
    "- while 'Gaussian component' ensures that only those pixels with intensities similar to that of central pixel(‘intensity neighbors’) are included to compute blurred intensity value.\n",
    "- Highly effective at 'noise removal' while 'preserving edges'.slower compared to other filters \n",
    "\n",
    "#@ blur=cv2.bilateralFilter(src,d,sigmaColor,sigmaSpace,borderType)\n",
    " * d: Diameter of each pixel neighborhood that is used during filtering(must be 'non-positive',computed from sigmaSpace)\n",
    " * sigmaColor:Filter sigma in color space.(larger value,tends larger areas of semi-equal color)\n",
    " * sigmaSpace: Filter sigma in coordinate space(larger value, influence each other as long as their colors are close enough)\n",
    " * borderType : border mode used to extrapolate pixels outside of the image\n",
    "    \n",
    ">>> blur=cv2.bilateralFilter(img,9,75,75) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Morphological Transformations :\n",
    " - Normally performed on 'binary images'& Needs two inputs:<br>\n",
    "       1) **original image** <br>\n",
    "       2) **structuring element|kernel(i.e. decides 'nature of operation')**<br> \n",
    " \n",
    "**Two basic morphological operators**\n",
    " 1) ***Erosion***<br>\n",
    " 2) ***Dilation***<br>\n",
    " \n",
    "**Other Variants**: <br>\n",
    " ***Opening*** , ***Closing*** , ***Gradient*** etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i) Erosion: 'erodes away boundaries' of foreground object(Always try to keep foreground in white)\n",
    " - kernel slides through image(as in 2D convolution).\n",
    " - pixel in original image(1|0) will be considered 1: if all pixels under kernel is 1\n",
    "                                                 else it is eroded(made to 0)\n",
    " - 'pixels near boundary' will be 'discarded' depending upon size of kernel.\n",
    " - So thickness|size of foreground object 'decreases|simply white region decreases' in image\n",
    "Used: Removing small white noises\n",
    "@ erosion=erode(src,kernel,anchor,iterations,borderType)\n",
    " * kernel: structuring element used for erosion(created using #getStructuringElement)\n",
    " * anchor: position of anchor within element(default value(-1,-1) means anchor @ element center)\n",
    " * iterations: number of times erosion is applied.\n",
    " * borderType: pixel extrapolation method\n",
    " * borderValue: border value in case of a constant border\n",
    "    \n",
    ">>> kernel=np.ones((5,5),np.uint8)\n",
    ">>> erosion=cv2.erode(img,kernel,iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii) Dilation : Just opposite of erosion.\n",
    " - pixel element is '1': if atleast one pixel under kernel is ‘1’\n",
    " - So it increases 'white region' in image / size of foreground object increases. \n",
    " - In cases of 'noise removal', erosion is followed by dilation.Cuz erosion removes white noises,but it also shrinks our object. \n",
    " - So we dilate it(Since noise is gone, they won’t come back) but our object area increases.\n",
    "Useful: Join broken parts of an object.\n",
    "\n",
    "@ dilation=cv2.dilate(src,kernel,anchor,iterations,borderType,borderValue)\n",
    " * kernel: structuring element used for dilation(created using #getStructuringElement)\n",
    " * anchor: position of anchor within element(default value(-1,-1) means anchor @ element center)\n",
    " * iterations: number of times dilation is applied.\n",
    " * borderType: pixel extrapolation method\n",
    " * borderValue: border value in case of a constant border\n",
    "    \n",
    ">>> kernel=np.ones((5,5),np.uint8)\n",
    ">>> dilation=cv2.dilate(img,kernel,iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iii) Opening : Refers to 'erosion followed by dilation'\n",
    "Used: Removing noise\n",
    "\n",
    "@ opening=cv2.morphologyEx(src,op,kernel,anchor,iterations,borderType,borderValue)\n",
    " * op: Type of a morphological operation(here op=cv2.MORPH_OPEN)\n",
    " * kernel: Structuring element(created using #getStructuringElement)\n",
    " * anchor: Anchor position with kernel (-ve values mean that anchor @ kernel center)\n",
    " * iterations: Number of times erosion & dilation are applied.\n",
    " * borderType: pixel extrapolation method\n",
    " * borderValue: border value in case of a constant border\n",
    " \n",
    ">>> kernel=np.ones((5,5),np.uint8)\n",
    ">>> opening=cv2.morphologyEx(img,cv2.MORPH_OPEN,kernel)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iv) Closing : Using cv2.morphologyEx()\n",
    " - Reverse of Opening,'dilation followed by erosion'\n",
    "Used: Closing small holes inside foreground objects|small black points on object\n",
    "@ closing=cv2.morphologyEx(src,op,kernel,anchor,iterations,borderType,borderValue)\n",
    " * op: Type of a morphological operation(here op=cv2.MORPH_CLOSE)\n",
    " * kernel: Structuring element(created using #getStructuringElement)\n",
    " * anchor: Anchor position with kernel (-ve values mean that anchor @ kernel center)\n",
    " * iterations: Number of times erosion & dilation are applied.\n",
    " * borderType: pixel extrapolation method\n",
    " * borderValue: border value in case of a constant border\n",
    " \n",
    ">>> kernel=np.ones((5,5),np.uint8)\n",
    ">>> closing=cv2.morphologyEx(img,cv2.MORPH_CLOSE,kernel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v) Morphological Gradient: It is difference between 'dilation' & 'erosion' of an image                               \n",
    "                               \n",
    "@ gradient=cv2.morphologyEx(img,cv2.MORPH_GRADIENT,kernel)                               \n",
    " * op: Type of a morphological operation(here op=cv2.MORPH_GRADIENT)\n",
    " * kernel: Structuring element(created using #getStructuringElement)\n",
    " * anchor: Anchor position with kernel (-ve values mean that anchor @ kernel center)\n",
    " * iterations: Number of times erosion & dilation are applied.\n",
    " * borderType: pixel extrapolation method\n",
    " * borderValue: border value in case of a constant border\n",
    " \n",
    ">>> kernel=np.ones((5,5),np.uint8)                                                              \n",
    ">>> gradient=cv2.morphologyEx(img,cv2.MORPH_GRADIENT,kernel) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi) Top Hat : It is difference between 'input image' & 'Opening of image'                               \n",
    "@ tophat=cv2.morphologyEx(img,cv2.MORPH_TOPHAT,kernel)\n",
    " * op: Type of a morphological operation(here op=cv2.MORPH_TOPHAT)\n",
    " * kernel: Structuring element(created using #getStructuringElement)\n",
    " * anchor: Anchor position with kernel (-ve values mean that anchor @ kernel center)\n",
    " * iterations: Number of times erosion & dilation are applied.\n",
    " * borderType: pixel extrapolation method\n",
    " * borderValue: border value in case of a constant border\n",
    "\n",
    ">>> kernel=np.ones((5,5),np.uint8)                              \n",
    ">>> tophat=cv2.morphologyEx(img,cv2.MORPH_TOPHAT,kernel)                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vii) Black Hat : It is difference between 'closing of input image' & 'input image'\n",
    "@ blackhat=cv2.morphologyEx(img,cv2.MORPH_BLACKHAT,kernel)\n",
    " * op: Type of a morphological operation(here op=cv2.MORPH_BLACKHAT)\n",
    " * kernel: Structuring element(created using #getStructuringElement)\n",
    " * anchor: Anchor position with kernel (-ve values mean that anchor @ kernel center)\n",
    " * iterations: Number of times erosion & dilation are applied.\n",
    " * borderType: pixel extrapolation method\n",
    " * borderValue: border value in case of a constant border\n",
    "\n",
    ">>> kernel=np.ones((5,5),np.uint8)                              \n",
    ">>> blackhat=cv2.morphologyEx(img,cv2.MORPH_BLACKHAT,kernel)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "### Structuring Element & Kernel: We can manually create 'structuring elements' help of Numpy(rectangular shape)\n",
    "   - elliptical/circular shaped kernel\n",
    "   -  pass shape & size of kernel,get desired kernel\n",
    "@ kernel=cv2.getStructuringElement(shape,ksize,anchor)\n",
    " * ksize: Size of structuring element(row,col)\n",
    " * anchor: Anchor position within element(default (-1,-1):anchor @ center)\n",
    " * shape: Element shape \n",
    "    - cv2.MORPH_RECT: Rectangle\n",
    "    - cv2.MORPH_ELLIPSE: ellipse\n",
    "    - cv2.MORPH_CROSS :cross - shaped\n",
    "        \n",
    ">>> kernel=cv2.getStructuringElement(cv2.MORPH_RECT,(5,5))  # Rectangular Kernel\n",
    ">>> cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(5,5))      # Elliptical Kernel\n",
    ">>> cv2.getStructuringElement(cv2.MORPH_CROSS,(5,5))        # Cross-shaped Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Gradients: \n",
    " - OpenCV provides three types of 'gradient filters'|'High-pass filters' <br>\n",
    "    ***Sobel X*** , ***Sobel Y*** , ***Laplacian***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i) Sobel & Scharr Derivatives : this Gausssian smoothing plus differentiation operation (more resistant to noise).\n",
    " - We can specify direction of derivatives to be taken(vertical|horizontal using 'yorder' & 'xorder' respectively) \n",
    " - We can specify 'size of kernel' by ksize(If ksize=-1: '3x3 Scharr filter used, gives better results' than '3x3 Sobel filter')\n",
    "@ sobel=Sobel(src,ddepth,dx,dy,ksize,scale,delta,borderType)\n",
    " * ddepth: output image depth\n",
    " * dx,dy: order of derivative x & y\n",
    " * ksize: size of extended Sobel kernel(must be odd)\n",
    " * scale: optional scale factor for computed derivative values(default None)\n",
    " * delta: optional delta value that is added to  results prior to storing them in dst.\n",
    " * borderType: pixel extrapolation method\n",
    "        \n",
    ">>> sobelx=cv2.Sobel(img,cv2.CV_64F,1,0,ksize=5)    ## Sobel X\n",
    ">>> sobely=cv2.Sobel(img,cv2.CV_64F,0,1,ksize=5)    ## Sobel Y   \n",
    "\n",
    "\n",
    "\n",
    "Output Data Type:\n",
    " - 'Black to White transition' taken as '+ve slope'(+ve value)\n",
    " - 'White to Black transition' taken as '-ve slope'(-ve value)\n",
    "So when you convert data to 'np.uint8',then all \" -ve slopes made zero \"(misses the edge)\n",
    "If we want to \"detect both edges\", keep \"output datatype to some higher\"forms(like cv2.CV_16S,cv2.CV_64F etc)\n",
    "take its absolute value & convert back to 'cv2.CV_8U' .\n",
    "\n",
    "\n",
    "sobelx8u=cv2.Sobel(img,cv2.CV_8U,1,0,ksize=5)     # Output dtype = cv2.CV_8U\n",
    "sobelx64f=cv2.Sobel(img,cv2.CV_64F,1,0,ksize=5) # O/p dtype=cv2.CV_64F.Then take its absolute and convert to cv2.CV_8U\n",
    "\n",
    "abs_sobel64f=np.absolute(sobelx64f)\n",
    "sobel_8u=np.uint8(abs_sobel64f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii) Laplacian Derivatives : Calculates Laplacian of the image given by relation.\n",
    " - each derivative is found using 'Sobel derivatives'\n",
    "@ laplacian=cv2.Laplacian(src,ddepth,ksize,scale,delta,borderType)\n",
    " * ddepth: output image depth\n",
    " * ksize: size of extended Sobel kernel(must be odd)\n",
    " * scale: optional scale factor for computed derivative values(default None)\n",
    " * delta: optional delta value that is added to  results prior to storing them in dst.\n",
    " * borderType: pixel extrapolation method\n",
    "     \n",
    ">>> laplacian=cv2.Laplacian(img,cv2.CV_64F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Canny Edge Detection:\n",
    " - Popular 'edge detection algorithm',was developed by 'John F. Canny' in 1986 \n",
    "It's multi-stage algorithm :\n",
    " \n",
    " 1) **Noise Reduction:**\n",
    "     'remove noise' using '5x5 Gaussian filter' from image.\n",
    " \n",
    " 2) **Finding Intensity Gradient of the Image:** \n",
    "    - Smoothened image,filtered with 'Sobel kernel'(both horizontal & vertical direction)\n",
    "    - Which get first derivative in \"horizontal direction(Gx)\" & \"vertical direction(Gy)\" \n",
    "    - From these 2 images,we can find 'Edge_Gradient' & 'direction for each pixel' \n",
    "                 \n",
    "                    Edge_Gradient(G) =sqrt{(Gx)^2 + (Gy)^2}\n",
    "\n",
    "                      Angle(theta) =tan(inv) (Gy/Gx)\n",
    "\n",
    "    - Gradient direction is always 'perpendicular to edges'.So,rounded to one of four angles(representing vertical,horizontal & two diagonal directions)\n",
    " \n",
    " 3) **Non-maximum Suppression** :\n",
    "    -After getting 'Edge_Gradient & direction',a full scan of image is done to 'remove any unwanted pixels',which maynt constitute edge\n",
    "    -At every pixel,'pixel is checked, if it is a local maximum' in its neighborhood in direction of gradient\n",
    "    -If it forms 'local maximum', it is considered for next stage, Else it is suppressed( put to 0)\n",
    "  - In short, result we get is a 'binary image with “thin edges” \n",
    "  \n",
    " 4) **Hysteresis Thresholding** :\n",
    "    This stage decides, which are all edges are really edges & which arent,that need 2 threshold values, 'minVal' & 'maxVal'. \n",
    "    -Edges with 'intensity gradient > maxVal': sure to be edges & 'intensity gradient < maxVal': sure to be non-edges(are discarded)\n",
    "    \n",
    "    -Those who lie between these 2 thresholds are classified 'edges' / 'non-edges' based on their connectivity.\n",
    "    -If they 'connected to “sure-edge” pixels': considered to be part of edges. Else they are discarded\n",
    "    This stage, 'removes small pixels noises' on assumption that edges are long lines.So we finally, get 'strong edges' in image\n",
    "\n",
    "Canny Edge Detection in OpenCV :\n",
    " - OpenCV puts all the above in single function'cv2.Canny()'\n",
    " - First argument is our input image. \n",
    "     Last argument is L2gradient which specifies the equation for finding gradient magnitude. If it is True,\n",
    "        it uses the equation mentioned above which is more accurate,\n",
    "otherwise it uses this function: . By default, it is False.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ edge=cv2.Canny(image,threshold1,threshold2,edges,apertureSize,L2gradient)\n",
    " * image:\n",
    " * threshold1:'minVal threshold'\n",
    " * threshold2:'maxVal threshold'\n",
    " * edges:output edge map, single channels 8-bit image, which has same size as image\n",
    " * apertureSize: 'Size of Sobel kernel' used for find image gradients (default = 3)\n",
    " * L2gradient: Specifies equation for finding Edge_Gradient(G)=|Gx|+|Gy| If True. (Default False) \n",
    "\n",
    ">>> edge=cv2.Canny(img,threshold1=100,threshold2=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img=cv2.imread(r\"C:\\Users\\joker\\Desktop\\opencv-course-master\\Resources\\Photos\\park.jpg\")\n",
    "cv2.imshow('image',img)  # Display image with CV\n",
    "\n",
    "rotation=cv2.getRotationMatrix2D((50,200),90,1)\n",
    "cv2.imshow('R',rotation)\n",
    "#cv2.imshow('2',scale2)\n",
    "#cv2.imshow('3',scale3)\n",
    "\n",
    "cv2.waitKey(0)  # Wait to End:Keyboard binding {0:wait until any key pressed}\n",
    "cv2.destroyAllWindows()            # Destroy all image windows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3|iii) Performance Measurement and Improvement Techniques\n",
    "- In image processing,we are dealing with large no. of operations per second\n",
    "- It is mandatory, providing correct solution in 'fastest manner'\n",
    " * To measure performance of code\n",
    " * Tips to improve performance of code\n",
    "\n",
    "                      a) Performance Measure\n",
    "\n",
    "##### Performance Measuring with OpenCV\n",
    "*'cv2.getTickCount'returns 'no. of clock-cycles after event A to event B'\n",
    "  - we can call it, before(event A) & after(event B) of the function execution(get 'clock-cycles' used to execute function)\n",
    "\n",
    "*'cv2.getTickFrequency' returns 'frequency of clock-cycles / no. of clock-cycles per secs',that find time of execution in secs \n",
    "\n",
    ">>> event_A=cv2.getTickCount() ## before code Execution\n",
    "## your code execution\n",
    ">>> event_B=cv2.getTickCount() ## after code Execution         \n",
    ">>> time=(event_A - event_B)/cv2.getTickFrequency()\n",
    ">>> time \n",
    "\n",
    "\n",
    "##### Performance Measuring with IPython\n",
    "\n",
    "\n",
    "                         b) Optimization \n",
    "\n",
    "##### Optimizationin OpenCV\n",
    "- Many OpenCV functions are optimized using 'SSE2','AVX'.Also contains unoptimized code also.\n",
    "- OpenCV runs optimized code if it is enabled, else it runs the unoptimized code.\n",
    "- We can use 'cv2.useOptimized()' to check if it is enabled/disabled & 'cv2.setUseOptimized()' to enable/disable it.\n",
    "\n",
    ">>> cv2.useOptimized() ## check if optimization is enabled\n",
    ">>> cv2.setUseOptimized(False)  ## Disable it\n",
    "\n",
    "\n",
    "\n",
    "                              c) Performance Optimization Techniques\n",
    "\n",
    "1) Avoid 'loops in Python' as possible,like double/triple loops etc(They are inherently slow)\n",
    "2) 'Vectorize algorithm/code' at max possible extent because Numpy & OpenCV are optimized for vector operations.\n",
    "3) Exploit cache coherence.\n",
    "4) 'Never make copies' of array unless needed(Try to use views instead as 'Array copying is costly operation') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h) \"\"\" Image Pyramids \"\"\"\n",
    "-when, we need to work with images of different resolution of same image\n",
    "-Create set of images with different resolution & search for object in all images,these set of images(different resolution) 'Image Pyramids',cuz they kept biggest image at bottom & smallest image at top \n",
    "Two kinds of Image Pyramids: (1) Gaussian Pyramid  (2) Laplacian Pyramids\n",
    "\n",
    "(1) Gaussian Pyramid:    \n",
    "-'Higher level(Low resolution)' formed by removing consecutive 'rows' & 'columns' in 'Lower level(higher resolution)' image \n",
    "-Then each pixel in 'higher level',formed by contribution from 5 pixels in underlying level with gaussian weights.\n",
    "so,'M*N' image becomes M/2 * N/2 image(area reduces to one-fourth of original area),called 'Octave' \n",
    "     i) 'CV2.pyrDown() :Lower Resolution '\n",
    "       #@ lower=pyrDown(src,dstsize,borderType)\n",
    "        >>> low=cv2.pyrDown(img)\n",
    "        >>> low1=cv2.pyrDown(low)\n",
    "        >>> low2=cv2.pyrDown(low1)\n",
    " \n",
    "     ii) 'CV2.pyrUp() :Higher Resolution '\n",
    "       #@ upper=pyrUp(src,dstsize,borderType)\n",
    "        >>> up=cv2.pyrUp(img)\n",
    "        >>> up1=cv2.pyrUp(up)\n",
    "        >>> up2=cv2.pyrUp(up1)\n",
    "\n",
    "(2) Laplacian Pyramid :\n",
    " -Formed from 'Gaussian Pyramids'.Laplacian pyramid images are like edge images only(Most elements are zeros)\n",
    " -Laplacian Pyramid is formed by difference between that level in Gaussian Pyramid & expanded version of its upper level in Gaussian Pyramid. \n",
    " -Used in image compression.\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i) \"\"\" Contours \"\"\"\n",
    "contour : A curve joining all continuous points(along the boundary), having same color|intensity\n",
    "          Usefull for 'shape analysis','object detection' & 'recognition'\n",
    "   * For better accuracy,'use binary images'.before finding contours,apply threshold/canny edge detection\n",
    "   * 'findContours()' modifies source image.we can store original image to another var\n",
    "   * Finding contours is like finding 'white object' from 'black background'(object to be found: in white & backgroud: in black)\n",
    "\n",
    "a)\n",
    "### Find Contours :\n",
    "  -'Contours' can be find using 'cv2.findContours()'.\n",
    "  - This can take 3 output image,contours(list),hierarchy(nd-array)\n",
    "     where image: Returns 'original image'\n",
    "           contours: Returns 'list of all contours'(Each contour is array(x,y) coordinates of boundary points of object)\n",
    "           hierarchy: Returns ''\n",
    "   Requirement & Steps(for finding contours):\n",
    "    1) convet image to Gray_Scale\n",
    "    2) Threshold \n",
    "    3) cv2.findContours()\n",
    "    \n",
    " #@ contours,hierarchy=cv2.findContours(image,mode,method,contours,hierarchy,offset)\n",
    "   * mode: Contour retrieval mode\n",
    "      -cv2.RETR_TREE:\n",
    "   * method: Contour approximation method\n",
    "      -cv2.CHAIN_APPROX_NONE : all the boundary points are stored\n",
    "      -cv2.CHAIN_APPROX_SIMPLE : \n",
    "   * contours:\n",
    "   * hierarchy:\n",
    "   * offset: Optional offset by which every contour point is shifted\n",
    "\n",
    ">>> img_gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)    ## img to Gray_Scale\n",
    ">>> ret,thresh=cv2.threshold(img_gray,127,255,0)     ## Threshold Of Image\n",
    ">>> contours,hierarchy=cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)  ## Find Contour\n",
    "\n",
    "\n",
    "### Draw Contours :\n",
    " - To draw contours, 'cv2.drawContours()' is used & output contours(list)\n",
    " - It can also be used to draw any shape provided you have its boundary points.\n",
    " #@ (image,contours,contourIdx,color,thickness,lineType,hierarchy,maxLevel,offset)\n",
    "   * image: Destination image.\n",
    "   * contours: All input contours(Each contour stored as point vector)\n",
    "   * contourIdx: Which contour to draw(If -1: draw all contour Else specific contour)\n",
    "   * color: Color of contours(in form of range(like, green=(0,255,255)))\n",
    "   * thickness: Thickness of lines, contours are drawn \n",
    "   * lineType: Line connectivity\n",
    "   * hierarchy: Optional information about hierarchy\n",
    "   * maxLevel: Maximal level for drawn contours\n",
    "   * offset: Optional contour shift parameter. Shift all drawn contours by specified space\n",
    "    \n",
    ">>> img=cv2.drawContours(img,contours,-1,(150,25,255),3)  \n",
    "\n",
    "\n",
    "b) Contour Features:\n",
    " ### Moments :\n",
    "  - It is weighted average of image pixels' intensities\n",
    "  - Usefull for describe area(total intensity) , centroid & its orientation\n",
    "  -'cv2.moments()' gives a dictionary of all moment values calculated\n",
    " #@ \n",
    " >>> m=cv2.moments(contours[0])\n",
    " ### Contour area: computes 'contour area'\n",
    "  -It is given by function 'cv2.contourArea()' / from moments:'M[‘m00’]'\n",
    "  #@ area=cv2.contourArea(contour,oriented)\n",
    "     * contour: contour Input vector of 2D points\n",
    "     * oriented: Oriented area flag(If True,function returns 'signed area value',depending on contour orientation)\n",
    "   >>> area=cv2.contourArea(contours[2])  \n",
    "\n",
    " ### Contour Perimeter :\n",
    "  -It is also called arc length. \n",
    "  -Perform using 'cv2.arcLength()' .\n",
    "    \n",
    "  #@ perimeter=cv2.arcLength(curve,closed)\n",
    "    * curve: Input vector \n",
    "    * closed: Flag indicating whether curve is closed / not\n",
    "   >>> perimeter=cv2.arcLength(contours[1],True)     \n",
    "\n",
    " ### Contour Approximation\n",
    "  - It approximates contour shape to another shape with less no. of vertices depending upon precision we specify\n",
    "  - \n",
    "    \n",
    " \n",
    "\n",
    " ### Convex Hull :\n",
    "   -checks curve for 'convexity defects' & 'corrects it'\n",
    "   convexity defects : \n",
    "  #@ hull= v2.convexHull(points,hull,clockwise,returnPoints]\n",
    "   * points: contours we pass into.\n",
    "   * hull: output, normally we avoid it.\n",
    "   * clockwise : Orientation flag(If True:output convex hull is oriented clockwise)\n",
    "   * returnPoints : If True:returns coordinates of  hull points(If False:returns indices of contour points corresponding to hull points)\n",
    "  >>> hull=cv2.convexHull(Contours)\n",
    "\n",
    " ###  Checking Convexity:\n",
    "   -Process to check, if curve is convex or not\n",
    "   -'cv2.isContourConvex()' return whether True / False\n",
    "  #@ C=cv2.isContourConvex(Contours)\n",
    "    \n",
    "   >>> C=cv2.isContourConvex(Contours[2])\n",
    "\n",
    " ### Bounding with Straight Rectangle :\n",
    " - Bounding contour with straight Rectngle\n",
    " >>> x,y,w,h=cv2.boundingRect(contours[10]) ## bounding specific contour\n",
    " >>> img=cv2.rectangle(img,(x,y),(x+w,y+h),(0,0,0),2) \n",
    " \n",
    " ### Bounding with Rotated Rectangle :\n",
    " - \n",
    " >>> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j) Histograms in OpenCV : \n",
    "\n",
    "histogram: Its a graph,that produces 'intensity distribution' of image. x-axis contains pixel values & y-axis contains corresponding no of pixels \n",
    " - histogram provides, intuition about 'contrast','brightness','intensity distribution' \n",
    "Bins: \n",
    "Dims: It is number of parameters for which we collect data\n",
    "Range:  It is the range of intensity values you want to measure\n",
    "\n",
    "    \n",
    "    i) Calculating & Drawing Histogram \n",
    "\n",
    "## calculating Histogram in OpenCV\n",
    "-\n",
    "#@ cv2.calcHist(images,channels,mask,histSize,ranges,hist,accumulate)\n",
    " * images : image of type uint8 or float32(must be in square brackets like [img])\n",
    " * channels : index of channel for which we calculate histogram(must be in square bracket)\n",
    "    for gray_scale:[0] , for color:[0],[1],[2] \n",
    " * mask : mask image,if we want to calculate histogram in specific region(if whole image mask:None)\n",
    " * histSize: Bin counts(must be in square brackets),for full range pass [256] \n",
    " * ranges :Ranges , normally[0,256]\n",
    ">>> hist=cv2.calcHist([img],[0],None,[256],[0,256])\n",
    "\n",
    "## Plotting Histogram Using Matplotlib\n",
    "\n",
    ">>> from matplotlib import pyplot as plt\n",
    ">>> color = ('b','g','r')\n",
    ">>> for i,col in enumerate(color):\n",
    "        hist=cv2.calcHist([img],[i],None,[256],[0,256])\n",
    "        plt.plot(hist,color=col)\n",
    "        plt.xlim([0,256])\n",
    "    plt.show()\n",
    "    \n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "ii) Histogram Equalization :\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k) \" Template Matching \"\n",
    "Process of searching & finding, location of 'template image' in larger image \n",
    "\n",
    "-'cv2.matchTemplate()':It simply slides template over image(as in 2D convolution) & compares template & patch image under template image.\n",
    "- Returns a grayscale image, where each pixel denotes how much does the neighbourhood of that pixel match with template.\n",
    "#@ tm=cv2.matchTemplate(image,templ,method,result,mask)\n",
    " * image: larger image(must be 8-bit / 32-bit)\n",
    " * templ: template image(mustnt > source_img & same data type)\n",
    " * result: Map of comparison results(o/p)\n",
    " * method: type of template maching mode\n",
    "    {cv2.TM_CCOEFF,cv2.TM_CCOEFF_NORMED,cv2.TM_CCORR,cv2.TM_CCORR_NORMED,cv2.TM_SQDIFF,cv2.TM_SQDIFF_NORMED}\n",
    " * mask: Mask of searched template. It must have the same datatype and size with templ\n",
    "    \n",
    "-If input image is of size(WxH) & template image is of size(wxh),output image will have size(W-w+1, H-h+1)\n",
    "-After getting templatematch('tm'),we can use 'cv2.minMaxLoc()' to find where is 'max_value & min_value'. \n",
    "#@ min_val,max_val,min_loc,max_loc=cv2.minMaxLoc(src,mask)\n",
    " * src: input single-channel array.\n",
    " * mask: optional mask used to select a sub-array\n",
    " * minVal: pointer to the returned minimum value; NULL is used if not required.\n",
    " * maxVal: pointer to the returned maximum value; NULL is used if not required.\n",
    " * minLoc: pointer to the returned minimum location (in 2D case); NULL is used if not required.\n",
    " * maxLoc: pointer to the returned maximum location (in 2D case); NULL is used if not required.\n",
    "\n",
    "-By Takeing 'max_value & min_value' as 'top-left corner of rectangle' & take template(w,h) as width & height of rectangle.\n",
    "-That is helpful for drawing rectangle in region of template.\n",
    "\n",
    "##### matching Single template \n",
    "import cv2\n",
    "import numpy as np\n",
    "img=cv2.imread(r\"A:\\Python_projects\\tianic_kaggle\\images\\titanic-disaster.jpg\")\n",
    "img_gr=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "temp=cv2.imread(r\"C:\\Users\\Sud()\\Desktop\\OCV_fle\\temp.jpg\",0)\n",
    "\n",
    "w,h=temp.shape[::-1]   ## provide (w,h) of template\n",
    "tm=cv2.matchTemplate(img_gr,temp,cv2.TM_CCOEFF_NORMED)   ## template matching\n",
    "\n",
    "min_val,max_val,min_loc,max_loc=cv2.minMaxLoc(tm)  ## \n",
    "\n",
    "# pt1=min_loc    ## for method:{cv2.TM_SQDIFF,cv2.TM_SQDIFF_NORMED} (pt1 for top_left points)\n",
    "pt1=max_loc      ## for remaining all  (pt1 for top_left points)\n",
    "pt2=(pt1[0]+w, pt1[1]+h)  ## (pt2 for bottom_right points)\n",
    "\n",
    "cv2.rectangle(img,pt1,pt2,(0,0,255),1)  ## setting up rectangle\n",
    "\n",
    "cv2.imshow('detected',img)\n",
    "#cv2.imshow('template',temp)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows() \n",
    "\n",
    "\n",
    "##### Matching multiple Templates\n",
    "import cv2\n",
    "import numpy as np\n",
    "img=cv2.imread(r\"A:\\Python_projects\\tianic_kaggle\\images\\titanic-disaster.jpg\")\n",
    "img_gr=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "temp=cv2.imread(r\"C:\\Users\\Sud()\\Desktop\\OCV_fle\\temp.jpg\",0)\n",
    "\n",
    "w,h=temp.shape[::-1]   ## provide (w,h) of template\n",
    "tm=cv2.matchTemplate(img_gr,temp,cv2.TM_CCOEFF_NORMED)   ## template matching\n",
    "\n",
    "threshold=0.95       ## setting threshold value\n",
    "loc=np.where(tm >= threshold)  ## greater than threshold value\n",
    "\n",
    "for pt1 in zip(*loc[::-1]):     ## finding locations of 'templates' in image (location (w,h))\n",
    "    cv2.rectangle(img,pt1,(pt1[0]+w,pt1[1]+h),(0,0,0),1) ##(W-w+1, H-h+1)\n",
    "\n",
    "cv2.imshow('detected',img)  ## matched image(template patched over image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l) Hough Transform\n",
    "\n",
    "\n",
    "a) Hough Line Transformation\n",
    "\n",
    "\n",
    "\n",
    "b) Hough Line Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img=cv2.imread(r\"C:\\Users\\Sud()\\Desktop\\OCV_fle\\lines.PNG\")\n",
    "#img_gray=cv2.imread(r\"C:\\Users\\Sud()\\Desktop\\OCV_fle\\lines.PNG\",0)\n",
    "cv2.namedWindow('e1',cv2.WINDOW_NORMAL)\n",
    "#cv2.namedWindow('e2',cv2.WINDOW_NORMAL)\n",
    "\n",
    "e1 = cv2.Canny(img,50,150,apertureSize = 3)\n",
    "#e2 = cv2.Canny(img_gray,50,150,apertureSize = 3)\n",
    "\n",
    "lines=cv2.HoughLines(e1,1,np.pi/180,50)\n",
    "for rho,theta in lines[0]:\n",
    "    a = np.cos(theta)\n",
    "    b = np.sin(theta)\n",
    "    x0 = a*rho\n",
    "    y0 = b*rho\n",
    "    x1 = int(x0 + 1000*(-b))\n",
    "    y1 = int(y0 + 1000*(a))\n",
    "    x2 = int(x0 - 1000*(-b))\n",
    "    y2 = int(y0 - 1000*(a))\n",
    "\n",
    "    cv2.line(img,(x1,y1),(x2,y2),(0,0,255),2)\n",
    "\n",
    "\n",
    "\n",
    "#cv2.imshow('e2',e2)\n",
    "cv2.imshow('detected',img)\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function HoughLines:\n",
      "\n",
      "HoughLines(...)\n",
      "    HoughLines(image, rho, theta, threshold[, lines[, srn[, stn[, min_theta[, max_theta]]]]]) -> lines\n",
      "    .   @brief Finds lines in a binary image using the standard Hough transform.\n",
      "    .   \n",
      "    .   The function implements the standard or standard multi-scale Hough transform algorithm for line\n",
      "    .   detection. See <http://homepages.inf.ed.ac.uk/rbf/HIPR2/hough.htm> for a good explanation of Hough\n",
      "    .   transform.\n",
      "    .   \n",
      "    .   @param image 8-bit, single-channel binary source image. The image may be modified by the function.\n",
      "    .   @param lines Output vector of lines. Each line is represented by a 2 or 3 element vector\n",
      "    .   \\f$(\\rho, \\theta)\\f$ or \\f$(\\rho, \\theta, \\textrm{votes})\\f$ . \\f$\\rho\\f$ is the distance from the coordinate origin \\f$(0,0)\\f$ (top-left corner of\n",
      "    .   the image). \\f$\\theta\\f$ is the line rotation angle in radians (\n",
      "    .   \\f$0 \\sim \\textrm{vertical line}, \\pi/2 \\sim \\textrm{horizontal line}\\f$ ).\n",
      "    .   \\f$\\textrm{votes}\\f$ is the value of accumulator.\n",
      "    .   @param rho Distance resolution of the accumulator in pixels.\n",
      "    .   @param theta Angle resolution of the accumulator in radians.\n",
      "    .   @param threshold Accumulator threshold parameter. Only those lines are returned that get enough\n",
      "    .   votes ( \\f$>\\texttt{threshold}\\f$ ).\n",
      "    .   @param srn For the multi-scale Hough transform, it is a divisor for the distance resolution rho .\n",
      "    .   The coarse accumulator distance resolution is rho and the accurate accumulator resolution is\n",
      "    .   rho/srn . If both srn=0 and stn=0 , the classical Hough transform is used. Otherwise, both these\n",
      "    .   parameters should be positive.\n",
      "    .   @param stn For the multi-scale Hough transform, it is a divisor for the distance resolution theta.\n",
      "    .   @param min_theta For standard and multi-scale Hough transform, minimum angle to check for lines.\n",
      "    .   Must fall between 0 and max_theta.\n",
      "    .   @param max_theta For standard and multi-scale Hough transform, maximum angle to check for lines.\n",
      "    .   Must fall between min_theta and CV_PI.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines=cv2.HoughLines(image,rho,theta,threshold,lines,srn,stn,min_theta,max_theta)\n",
    "*image : 8-bi (single-channel binary)\n",
    "* rho : \n",
    "* theta \n",
    "* threshold \n",
    "* lines \n",
    "* srn \n",
    "* stn \n",
    "* min_theta \n",
    "* max_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Feature Detection & Description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Machine Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  7) Computational Photography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Camera Calibration & 3D Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Video Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  OpenCV-Python Bindings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
