{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T19:22:04.121115Z",
     "start_time": "2021-09-06T19:22:03.864017Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "cv2.__version__\n",
    "img=cv2.imread(r\"C:\\Users\\JokeRR\\Referances\\python_projects\\Number-plate Detection\\image3.jpg\",1) #,cv2.IMREAD_GRAYSCALE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gui Features: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Getting Started with Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T05:51:07.749754Z",
     "start_time": "2021-09-02T05:51:07.725294Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Read Image :\n",
    "img=cv2.imread(r\"C:\\Users\\JokeRR\\Referances\\python_projects\\Number-plate Detection\\image3.jpg\",cv2.IMREAD_COLOR) \n",
    "\n",
    "# Color channels \n",
    "cv2.IMREAD_COLOR|(1) : Colored\n",
    "cv2.IMREAD_GRAYSCALE|(0) : Gray-scale\n",
    "cv2.IMREAD_UNCHANGED|(-1) : Alpha channel\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-02T05:51:08.195Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Window Size:\n",
    "cv2.namedWindow('window_name',cv2.WINDOW_NORMAL)   # Resizable window\n",
    "cv2.namedWindow('window_name',cv2.WINDOW_AUTOSIZE) # Auto-Sized window\n",
    "\n",
    "\n",
    "## Display Image: Using cv2.imshow()\n",
    "cv2.imshow('image_name2',img)  # Show image  @imshow('window_name',image)\n",
    "cv2.waitKey(0)                 # Wait to End:Keyboard binding @cv2.waitKey(any key)\n",
    "cv2.destroyAllWindows()        # Destroy all image windows\n",
    "\n",
    "## Using Matplotlib :\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img,cmap='gray',interpolation='bicubic')# show Image\n",
    "plt.xticks([]),plt.yticks([])                      # hide tick values on X & Y\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-02T05:51:09.170Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Write Image:\n",
    "cv2.imread(r'C:\\Users\\joker',1)   #@ cv2.imread(path,color_mode) \n",
    "\n",
    "\n",
    "# Save when button pressed:\n",
    "cv2.imshow(\"Display window\",img)\n",
    "k=cv2.waitKey(0)\n",
    "if k==ord(\"s\"):\n",
    "    cv2.imwrite(\"starry_night.png\",img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-02T05:51:09.872Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Basic image properties\n",
    "img.size    # Size of image(Total Pixel Size)\n",
    "img.shape   # Shape of image\n",
    "img.dtype   # data type of image\n",
    "img[11,433] # Access Image Pixel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Getting Started with Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Drawing Shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Line \"\"\" \n",
    "Draws Line between pt1 & pt2\n",
    "@ line(img,pt1,pt2,color,thickness,lineType,shift)\n",
    " * img: Image.\n",
    " * pt1: 1st point of line ; pt2: 2nd point of line\n",
    " * color: Line color\n",
    " * thickness: Line thickness.\n",
    " * lineType: Type of the line\n",
    " * shift: Number of fractional bits in point coordinates.\n",
    "\n",
    "cv2.line(img,(100,250),(300,400),(255,0,255),3) \n",
    "\n",
    "\n",
    "\"\"\" Polygon \"\"\"\n",
    "Draws one or more polygonal curves.\n",
    "@ polylines(img,pts,isClosed,color,thickness,lineType,shift)\n",
    " * pts: Array of polygonal curves.\n",
    " * isClosed: whether drawn polylines are closed or not\n",
    " * shift: Number of fractional bits in vertex coordinates\n",
    "\n",
    "coordinates=np.array([[10,5],[20,30],[70,20],[50,10]],np.int32).reshape((-1,1,2))  ## set vertices\n",
    "cv2.polylines(img,[coordinates],True,(0,255,255))\n",
    "\n",
    "\n",
    "\"\"\" Circle \"\"\"\n",
    "Draws simple|filled circle with given center & radius\n",
    "@ circle(img, center, radius, color[, thickness[, lineType[, shift]]]) -> img\n",
    " * center: Center of the circle.\n",
    " * radius: Radius of the circle.\n",
    " * shift: Number of fractional bits in coordinates of center & in radius value.\n",
    "\n",
    "cv.circle(img,(415,215),20,(0,0,255),1)\n",
    "\n",
    "\n",
    "\"\"\" Ellipse \"\"\"\n",
    "Draws simple|thick elliptic arc|fills ellipse sector\n",
    "@ ellipse(img,center,axes,angle,startAngle,endAngle,color,thickness,lin * \n",
    " * center: Center of ellipse\n",
    " * axes: Half of size of ellipse main axes\n",
    " * angle: Ellipse rotation angle in degrees\n",
    " * startAngle: Starting angle of elliptic arc in degrees.\n",
    " * endAngle: Ending angle of elliptic arc in degrees.\n",
    " * shift: Number of fractional bits in coordinates of center & values of axes.\n",
    "    \n",
    "cv2.ellipse(img,(256,256),(100,50),0,0,180,(0,0,255),-1)\n",
    "\n",
    "\n",
    "\"\"\" Rectangle \"\"\"\n",
    "draws outline|filled rectangle whose two opposite corners are pt1 & pt2\n",
    "@ rectangle(img,pt1,pt2,color,thickness,lineType,shift)\n",
    " * pt1: Vertex of rectangle ; pt2: Vertex of rectangle opposite to pt1 \n",
    " * shift: Number of fractional bits in the point coordinates.\n",
    "    \n",
    "cv.rectangle(img,(0,0),(img.shape[1]//2,img.shape[0]//2),(0,255,0),-1)\n",
    "\n",
    "\n",
    "\"\"\" Text \"\"\"\n",
    "Draws a text string\n",
    "@ putText(img,text,org,fontFace,fontScale,color,thickness,lineType,bottomLeftOrigin)\n",
    " * text: Text string to be drawn.\n",
    " * org: Bottom-left corner of text string in image.\n",
    " * fontFace: Font type\n",
    " * fontScale: Font scale factor (multiplied by font-specific base size)\n",
    " * bottomLeftOrigin: True: image data origin is at bottom-left corner; False: top-left corner\n",
    "\n",
    "text=\"LoL !!!!!! \"\n",
    "cv.putText(img,text,(0,225),cv.FONT_HERSHEY_TRIPLEX,1.0,(0,255,0),2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Mouse as Paint-Brush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Trackbar as the Color Palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Core Operations :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Basic Operations on images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T15:59:57.850643Z",
     "start_time": "2021-09-02T15:59:57.834627Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Access & Modify  Pixel & color-channel values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Access & Modify  Pixel & color-channel values : For Colored-image\n",
    "\"\"\" Access Pixel Values \"\"\" Pixel-values accessed by its 'row' & 'column'\n",
    "img[100,100]           # Intensity of [Blue,Red,Green]  @ image[row,col]\n",
    "\n",
    "  \n",
    "\"\"\" Access Colored-values \"\"\"  Specific color-channelel value(Blue,Green,Red) can be Accessed\n",
    "channel={0:Blue|1:green|2:Red} \n",
    "\n",
    "img[100,100,0]        # Blue-channel value @ img[row,col,channel]  \n",
    "img.item(100,100,0)   # Blue-channel value @ img.item(row,col,channel)\n",
    "\n",
    "\n",
    "\"\"\" Modify Pixel Value \"\"\" Modify pixel-value\n",
    "img[100,100]=[220,255,100]    # Modify pixel-values @ img[row,column]=[Blue_value,Green_value,Red_value]\n",
    "\n",
    "\n",
    "\"\"\" Modify Colored-values \"\"\" Modifyy specific color-channelel value(Blue,Green,Red)\n",
    "channel={0:Blue|1:green|2:Red} \n",
    "\n",
    "img[100,100,0]=233           # Modify Blue-channel value @ img[row,col,channel]=value  \n",
    "img.item(100,100,0)=233      # Modify Blue-channel value @ img.item(row,col,channel)=value\n",
    "img.itemset((10,10,0),211)   # Modify Blue-channel value @ img.itemset((row,col,chanel),value)\n",
    "\n",
    "\n",
    "### Access & Modify  Pixel & color-channel values : For Gray-Scaled image\n",
    "\"\"\" Access Pixel Values \"\"\" Pixel-values accessed by its 'row' & 'column'\n",
    "img[100,100]           # Access pixel-value  @ image[row,col]\n",
    "img.item(100,100)      # Access pixel-value  @ image.item[row,col]\n",
    "\n",
    "\"\"\" Modify Pixel Value \"\"\" Modify pixel-value\n",
    "img[100,100]=220           # Modify pixel value @ img[row,column]=value\n",
    "img.item(100,100)          # Modify pixel value @ image.item[row,col]=value\n",
    "img.itemset((10,10),211)   # Modify pixel value @ img.itemset((row,col),value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Set Region of Image (ROI) :\n",
    "* ROI: `Refers to choosing specific Region of Image,that obtained using 'Numpy Indexing'`\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T16:34:21.156063Z",
     "start_time": "2021-09-02T16:34:16.033334Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Selecting Region:\n",
    "region=img[200:300,50:200]  # @ region=img[start_row:end_row,start_col:end_col]\n",
    "\n",
    "## Adding Region:\n",
    "img[:240,100:260]=region    # @ img[start_row:end_row,start_col:end_col]=region\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Split & Merge Color channel :\n",
    "***cv2.split() :*** `split color channels`\n",
    "\n",
    "***cv2.merge() :*** `Merge split-colored channels`\n",
    "\n",
    "`cv2.split() & cv2.merge() are very time taking(we can use alternative)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Split Color-channels \"\"\"\n",
    "@ split(m,mv)\n",
    " * m: i/p multi-channel array\n",
    " * mv: o/p vector of arrays (arrays reallocated if needed)\n",
    "\n",
    "B,G,R=cv2.split(img)\n",
    "# or\n",
    "blue=img[:,:,0]\n",
    "green=img[:,0,:]\n",
    "red=img[0,:,:]\n",
    "\n",
    "\n",
    "\"\"\" Merge color-Channel \"\"\"\n",
    "@ merge(mv,dst)\n",
    " * mv: i/p vector of matrices to be merged (all matrices must same size & same depth)\n",
    " * dst: o/p array of same size & same depth as mv[0] (No. of channels = total number of channels in matrix array)\n",
    "\n",
    "merged=cv2.merge([B,G,R])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Image Border :\n",
    "* We can add border of image(like photo frame) , zero padding etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@ cv2.copyMakeBorder(src,top,bottom,left,right,borderType,value)\n",
    " * src:Input image(where to draw border)\n",
    " * top,bottom,left,right: border width(in number of pixel)\n",
    " * borderType: Border type\n",
    "    cv2.BORDER_CONSTANT - Constant colored border(value=color)\n",
    "    cv2.BORDER_REFLECT - Mirror reflection of border elements (fedcba|abcdefgh|hgfedcb)\n",
    "    cv2.BORDER_REFLECT_101 | cv2.BORDER_DEFAULT - Same as above, but with a slight change, like this : gfedcb|abcdefgh|gfedcba\n",
    "    cv2.BORDER_REPLICATE - Last element is replicated throughout, like this: aaaaaa|abcdefgh|hhhhhhh\n",
    "    cv2.BORDER_WRAP -it will look like this (cdefgh|abcdefgh|abcdefg)\n",
    " * value:color value for cv2.BORDER_CONSTANT ([255,0,0]=B|[0,255,0]=G|[0,0,255]=R)\n",
    "\n",
    "\n",
    "border=cv2.copyMakeBorder(img,10,10,10,10,cv2.BORDER_CONSTANT,value=[255,0,0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Arithmetic Operations on Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Image Addition \"\"\" Adds multiple images or scalar value\n",
    "Note: Imges must be of 'same size','same type' & 'same depth'\n",
    "\n",
    "@ add(src1,src2,dst,mask,dtype)\n",
    " * src1: 1st input array|scalar ; src2: 2nd input array|scalar.\n",
    " * dst: o/p array (same size & no. of channels as i/p array; depth: defined by dtype or src1/src2)\n",
    " ** mask: specifies elements of o/p array to be changed (mask: 8-bit single channel array)\n",
    " ** dtype:depth of o/p array \n",
    "\n",
    "var=cv2.add(img1,img2)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Image Blending \"\"\" \n",
    "- Image addition but 'different weights'can be given,that controlls blending(transparency) \n",
    "- Calculates weighted sum of two images \n",
    "Formula: added=ima1.alpha+img2.beta+gamma\n",
    "\n",
    "@ cv2.addWeighted(src1,alpha,src2,beta,gamma) \n",
    " * alpha: weight for 1st image, perform a cool transition (range from 0-1)\n",
    " * beta: weight for 2st image, perform a cool transition (range from 0-1) \n",
    " * gamma: weight/scalar added to blend,controls Exposure \n",
    "    \n",
    "dst=cv2.addWeighted(img,0.4,img1,0.6,0)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Bitwise Operations \"\"\" AND, OR, NOT & XOR operations   \n",
    "- Useful for 'extracting any part of image','defining' & 'working with non-rectangular ROI'\n",
    "\n",
    "rectangle=cv.rectangle(blank.copy(),(30,30),(370,370),255,-1)\n",
    "circle=cv.circle(blank.copy(),(200,200),200,255,-1)\n",
    "\n",
    "## bitwise AND --> intersecting regions\n",
    "bitwise_and=cv.bitwise_and(rectangle,circle)\n",
    "\n",
    "## bitwise OR --> non-intersecting and intersecting regions\n",
    "bitwise_or=cv.bitwise_or(rectangle,circle)\n",
    "\n",
    "## bitwise XOR --> non-intersecting regions\n",
    "bitwise_xor=cv.bitwise_xor(rectangle,circle)\n",
    "\n",
    "## bitwise NOT\n",
    "bitwise_not=cv.bitwise_not(circle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Performance Measurement and Improvement Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "https://docs.opencv.org/4.5.2/d7/d16/tutorial_py_table_of_contents_core.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Processing :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Changing Colorspaces :\n",
    " * `Converts images from one color-space to another(BGR -> Gray, BGR -> HSV etc.)`\n",
    " * `OpenCV supports 150 color-space conversion methods available in OpenCV `\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Color  Spaces & Ranges \"\"\":\n",
    "- In Color Images(BGR) -> [Blue,Green,Red] (all ranges from [0,255])\n",
    "- In HSV -> [Hue,Saturation,Value] (h range [0,179] & s,v range[0,255])\n",
    "- In Gray -> [Shades of gray] (Ranges[0-255])\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" List of Color-Conversion Flags \"\"\"\n",
    "flags=[i for i in dir(cv2) if i.startswith('COLOR_')]\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Changing color space \"\"\": convert images from one color space to another(BGR -> Gray|BGR -> HSV)\n",
    "@ cvtColor(src,code,dst,dstCn)\n",
    " * src: I/p image '8-bit unsigned','16-bit unsigned','single-precision floating-point'\n",
    " * dst: O/p image of same size & depth as src\n",
    " * code: color space conversion code|flags\n",
    " * dstCn: No. of channels in destination image  if 0:number of channels is derived automatically from src & code\n",
    "    \n",
    "gray=cv.cvtColor(img,cv.COLOR_BGR2GRAY)   # BGR to Grayscale\n",
    "hsv=cv.cvtColor(img,cv.COLOR_BGR2HSV)     # BGR to HSV\n",
    "lab=cv.cvtColor(img,cv.COLOR_BGR2LAB)     # BGR to L*a*b\n",
    "rgb=cv.cvtColor(img,cv.COLOR_BGR2RGB)     # BGR to RGB\n",
    "lab_bgr=cv.cvtColor(lab,cv.COLOR_LAB2BGR) # HSV to BGR\n",
    "# can't  convert gray-scale to HSV directly: we have to convert gray->BGR->HSV \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Object Tracking \n",
    "https://docs.opencv.org/4.5.2/df/d9d/tutorial_py_colorspaces.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric Transformations of Images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array Operations(mathematical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image=image+20      # vector Addition\n",
    "\n",
    "\n",
    "new_image=10*image      # vector multiplication \n",
    "\n",
    "\n",
    "noise=np.random.normal(0,20,(rows,cols,3)).astype(np.uint8)\n",
    "new_image=image+noise                   ## adding random noise\n",
    "\n",
    "\n",
    "noise=np.random.normal(0,20,(rows,cols,3)).astype(np.uint8)\n",
    "new_image = image*Noise                  ## multiplying noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Operations(mathematical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling :\n",
    "* Used for 'Resizing image'\n",
    "* Size of image can be specified manually or scaling factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ cv2.resize(src,dsize,dst,fx,fy,interpolation)\n",
    " * dsize: output image size\n",
    " * dst: output image\n",
    " * fx,fy: scale along x & y direction\n",
    " * interpolation: type of scaling\n",
    "    cv2.INTER_AREA: Shrink \n",
    "    cv2.INTER_CUBIC: Slow\n",
    "    cv2.INTER_LINEAR: Zooming\n",
    "\n",
    "    \n",
    "scale=cv2.resize(img,None,fx=2,fy=2,interpolation=cv2.INTER_CUBIC)    \n",
    "#or\n",
    "height,width=img.shape[:2]\n",
    "res=cv2.resize(img,(2*width,2*height),interpolation=cv2.INTER_AREA)\n",
    "#or\n",
    "res=cv.resize(img,(500,500),interpolation=cv.INTER_CUBIC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T06:19:09.704516Z",
     "start_time": "2021-09-03T06:19:09.691628Z"
    }
   },
   "source": [
    "### Translation :\n",
    " - Shifting object Location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "### warpAffine:Takes 2x3 transformation matrix as input\n",
    "@ warpAffine(src,M,dsize,dst,flags,borderMode,borderValue)\n",
    " * src: input image\n",
    " * dst: output image (size dsize & same type as src) \n",
    " * M: \\f$2\\times 3\\f$ transformation matrix.\n",
    " * dsize: size of output image.\n",
    " * flags: combination of interpolation methods\n",
    " * borderMode: pixel extrapolation method\n",
    "    borderMode=BORDER_TRANSPARENT: pixels in destination image corresponding to \"outliers\" in source image are not modified by function\n",
    " * borderValue: value used in case of constant border (default=0)\n",
    "    \n",
    "row,col=img.shape[:2]\n",
    "x,y=-100,100\n",
    "transMat=np.float32([[1,0,x],[0,1,y]])\n",
    "translated=cv2.warpAffine(img,transMat,(col,row))\n",
    "\n",
    "\n",
    "### warpPerspective: Takes 3x3 transformation matrix as input\n",
    "https://docs.opencv.org/4.5.2/da/d6e/tutorial_py_geometric_transformations.html\n",
    "\n",
    "    \n",
    "### Positions:\n",
    "   -x --> Left  # -y --> Up\n",
    "   x --> Right  # y --> Down\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotation :\n",
    "- Rotate an image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ getRotationMatrix2D(center,angle,scale)\n",
    " * center: Center of rotation in source image\n",
    " * angle: Rotation angle in degrees.\n",
    "    +ve values: counter-clockwise rotation (coordinate origin is assumed to be top-left corner)\n",
    " * scale: Isotropic scale factor\n",
    "    \n",
    "row,col=img.shape[:2]\n",
    "rotate=cv2.getRotationMatrix2D(center=(cols/2.0,rows/2.0),angle=90,scale=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Affine Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T06:41:27.009227Z",
     "start_time": "2021-09-03T06:41:26.988800Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function getAffineTransform:\n",
      "\n",
      "getAffineTransform(...)\n",
      "    getAffineTransform(src, dst) -> retval\n",
      "    .   @overload\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@ getAffineTransform(src,dst)\n",
    " * src: input image\n",
    " * dst: output image (size dsize & same type as src) \n",
    "    \n",
    "https://docs.opencv.org/4.5.2/da/d6e/tutorial_py_geometric_transformations.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Perspective Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "https://docs.opencv.org/4.5.2/da/d6e/tutorial_py_geometric_transformations.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Thresholding :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T06:46:25.861583Z",
     "start_time": "2021-09-03T06:46:25.838270Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "### Simple Thresholding:\n",
    "* If ***pixel_value > \"threshold\":*** `Assigns 1(may white)` `else Assigns 0(may Black)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@ ret,thre=cv2.threshold(src,thresh,maxval,type)        \n",
    " * scr: Only Gray_scaled Image\n",
    " * thresh: threshold value to consider,used to classify pixel(pixel>thre:1 / 0) \n",
    " * maxval:Value to given, if pixel value is more than(sometimes less than) threshold\n",
    " * type: Type of threshold   \n",
    "    cv2.THRESH_BINARY -\n",
    "    cv2.THRESH_BINARY_INV - \n",
    "    cv2.THRESH_TRUNC - \n",
    "    cv2.THRESH_TOZERO - \n",
    "    cv2.THRESH_TOZERO_INV - \n",
    "\n",
    "    \n",
    "ret,thre=cv2.threshold(img,127,255,cv2.THRESH_BINARY)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Adaptive Thresholding \n",
    " * Calculate threshold for small regions of image\n",
    " * returns 'different thresholds' for 'different regions' of same image \n",
    " * It gives us better results for images with varying illumination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@ ret,thre=cv2.adaptiveThreshold(src,maxValue,adaptiveMethod,thresholdType,blockSize,C)\n",
    " * src : gray_scaled image\n",
    " * maxValue : Value to given, if pixel value is more than(sometimes less than) threshold \n",
    " * blockSize : decides the size of neighbourhood area.\n",
    " * C : (constant)which is subtracted from the mean or weighted mean calculated\n",
    " * adaptiveMethod : decides how thresholding value is calculated\n",
    "     cv2.ADAPTIVE_THRESH_MEAN_C :threshold value is mean of neighbourhood area.\n",
    "     cv2.ADAPTIVE_THRESH_GAUSSIAN_C :threshold value is weighted sum of neighbourhood values where weights are 'gaussian window'\n",
    " * thresholdType : Type of threshold   \n",
    "     cv2.THRESH_BINARY -\n",
    "     cv2.THRESH_BINARY_INV - \n",
    "     cv2.THRESH_TRUNC - \n",
    "     cv2.THRESH_TOZERO - \n",
    "     cv2.THRESH_TOZERO_INV -\n",
    "    \n",
    "\n",
    "thre=cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,11,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Otsu’s Binarization\n",
    "* Automatically calculates 'threshold' from image histogram for 'bimodal image' \n",
    " \n",
    "* ***Working:***    \n",
    "    * For Bimodal images,it tries to find 'threshold_value(t)'(minimizes 'weighted within-class variance')\n",
    "* ***Implemention:*** \n",
    "    * Here,'cv2.threshold()' used,but uses extra flag'cv2.THRESH_OTSU' & (for thresh=0) \n",
    "    * So, Algo finds 'optimal threshold_value' & returns 'ret'\n",
    "    * If 'Otsu thresholding' not used,then 'ret' returns 'simple threshold value'\n",
    " Note: For images which are not bimodal,binarization won’t be accurate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@ ret,thre=cv2.threshold(src,thresh,maxval,type+cv2.THRESH_OTSU)        \n",
    " * scr: Only Gray_scaled Image\n",
    " * thresh:Used to classify pixel(in this case thres=0) \n",
    " * maxval:Value to given, if pixel value is more than(sometimes less than) threshold\n",
    " * type: Type of threshold   \n",
    "    cv2.THRESH_BINARY -\n",
    "    cv2.THRESH_BINARY_INV - \n",
    "    cv2.THRESH_TRUNC - \n",
    "    cv2.THRESH_TOZERO - \n",
    "    cv2.THRESH_TOZERO_INV - \n",
    " \n",
    "\n",
    "ret,thre=cv2.threshold(img,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing Images :\n",
    " * Images can be 'filtered' with various 'low-pass filters(LPF)',that remove noise/blurr image\n",
    " * It is useful for removing noise \n",
    " * Removes high-frequency content(noise,edges),that results blurred edges when this is filter is applied.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Image Filtering: Using 2D Convolution \n",
    " - Images can be 'filtered' with various 'low-pass filters(LPF)',that remove noise/blurr image\n",
    " - 'cv2.filter2D()' convolves a kernel with an image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T06:58:45.482929Z",
     "start_time": "2021-09-03T06:58:37.179633Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@ filter=filter2D(src,ddepth,kernel)\n",
    " * ddepth: desired depth of ' output image'\n",
    " * kernel: convolution kernel(correlation kernel),a single-channel floating point\n",
    "    \n",
    "kernel=np.ones((5,5),np.float32)/25\n",
    "filters=cv2.filter2D(img,-1,kernel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Image Blurring(Smoothing Image): Using Averging\n",
    " - Done by convolving image with a 'normalized box filter'\n",
    " - Simply takes 'average of all pixels under kernel' & replaces 'central element with this average'\n",
    " - 'cv2.blur() ' / ' cv2.boxFilter() ' can be used for this operation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T07:01:07.678120Z",
     "start_time": "2021-09-03T07:00:32.064653Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### boxFilter() \n",
    "@ filter=cv2.boxFilter(src,ddepth,(ksize),normalize,borderType)\n",
    " * ddepth:  output image depth(-1 to use src.depth())\n",
    " * ksize: blurring kernel size\n",
    " * anchor: anchor point;anchor is at the kernel center . default value (-1,-1)\n",
    " * normalize: specifying whether 'kernel is normalized by its area or nt' (True:enable/False:Disable)\n",
    " * borderType: border mode used to extrapolate pixels outside of the image\n",
    "    \n",
    "filters=cv2.boxFilter(img,ksize=(5,5),normalize=False)  ## using cv2.blur()\n",
    "\n",
    "\n",
    "### cv2.blur()\n",
    "@ filter=cv2.blur(src,(ksize))    \n",
    " * ksize: blurring kernel size\n",
    "\n",
    "filters=cv2.blur(img,(5,5))  ## using cv2.blur()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Gaussian Blurring : 'Gaussian kernel' is used, instead of 'box filter\n",
    " - Highly effective in removing Gaussian noise from image.\n",
    " - Takes width & height of kernel (i.e. +ve & odd value)\n",
    " - specify std in X & Y directions, sigmaX & sigmaY respectively(sigmaX is specified)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@ blur=GaussianBlur(src,ksize,sigmaX,sigmaY,borderType)    \n",
    " * ksize : Gaussian kernel size(ksize.width,ksize.height)    (+ve & odd value)\n",
    " * sigmaX,sigmaY : Gaussian kernel std in X,Y direction(if sigmaY=0,then equals to sigmaX)\n",
    " * borderType : border mode used to extrapolate pixels outside of the image \n",
    "    \n",
    "blur=cv2.GaussianBlur(img,(5,5),0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Median Blurring  : Using cv2.medianBlur()\n",
    "- Computes 'median of all pixels' underkernel window & 'central pixel' is replaced with this 'median value'\n",
    "- Central element is always replaced by some pixel value in image.\n",
    "- Highly effective in removing 'salt-pepper noise'(kernel size must be +ve & odd )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@ blur=cv2.medianBlur(src,ksize)\n",
    " * ksize: aperture linear size(must be odd & +ve & greater than 1,  e.g: 3,5,7)\n",
    "\n",
    "blur=cv2.medianBlur(img,5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Bilateral Filtering : Using cv2.bilateralFilter()\n",
    "- Uses 'Gaussian filter' in space domain,butalso uses one more(multiplicative) Gaussian filter component \n",
    "- 'Gaussian function of space' makes sure that only pixels,'spatial neighbors' are considered for filtering\n",
    "- while 'Gaussian component' ensures that only those pixels with intensities similar to that of central pixel(‘intensity neighbors’) are included to compute blurred intensity value.\n",
    "- Highly effective at 'noise removal' while 'preserving edges'.slower compared to other filters \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@ blur=cv2.bilateralFilter(src,d,sigmaColor,sigmaSpace,borderType)\n",
    " * d: Diameter of each pixel neighborhood that is used during filtering(must be 'non-positive',computed from sigmaSpace)\n",
    " * sigmaColor:Filter sigma in color space.(larger value,tends larger areas of semi-equal color)\n",
    " * sigmaSpace: Filter sigma in coordinate space(larger value, influence each other as long as their colors are close enough)\n",
    " * borderType : border mode used to extrapolate pixels outside of the image\n",
    "    \n",
    "\n",
    "blur=cv2.bilateralFilter(img,9,75,75) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Morphological Transformations :\n",
    "`Normally performed on 'binary images'& Needs two inputs`:\n",
    "       1) **original image** <br>\n",
    "       2) **structuring element|kernel(i.e. decides 'nature of operation')**<br> \n",
    " \n",
    "**Two basic morphological operators**\n",
    " 1) ***Erosion***<br>\n",
    " 2) ***Dilation***<br>\n",
    " \n",
    "**Other Variants**: <br>\n",
    " ***Opening*** , ***Closing*** , ***Gradient*** etc.\n",
    " \n",
    "https://docs.opencv.org/4.5.2/d9/d61/tutorial_py_morphological_ops.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Erosion:\n",
    " * `Erodes away boundaries of foreground object(Always try to keep foreground in white)`\n",
    " * `kernel slides through image(as in 2D convolution)`\n",
    " * pixel in image(1|0) will be considered \n",
    " `1: if all pixels under kernel=1` &  `else it is eroded(made to 0)`\n",
    " * 'pixels near boundary' will be 'discarded' depending upon size of kernel.\n",
    " * So thickness|size of foreground object 'decreases|simply white region decreases' in image\n",
    "\n",
    "***Used:*** Removing small white noises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T16:46:55.828324Z",
     "start_time": "2021-09-04T16:46:45.514745Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@ erosion=erode(src,kernel,anchor,iterations,borderType)\n",
    " * kernel: structuring element used for erosion(created using #getStructuringElement)\n",
    " * anchor: position of anchor within element(default value(-1,-1) means anchor @ element center)\n",
    " * iterations: number of times erosion is applied.\n",
    " * borderType: pixel extrapolation method\n",
    " * borderValue: border value in case of a constant border\n",
    "    \n",
    "kernel=np.ones((5,5),np.uint8)\n",
    "erosion=cv2.erode(img,kernel,iterations=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Dilation : Just opposite of erosion.\n",
    " * pixel Value=1: if atleast one pixel under kernel is ‘1’\n",
    " * So it increases 'white region' in image/size of foreground object increases. \n",
    " * In cases of 'noise removal',erosion is followed by dilation.Cuz erosion removes white noises,but it also shrinks our object \n",
    " * So we dilate it(Since noise is gone, they won’t come back) but our object area increases\n",
    "\n",
    "***Useful***: Join broken parts of an object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T17:28:17.538122Z",
     "start_time": "2021-09-04T17:28:05.239446Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@ dilation=cv2.dilate(src,kernel,anchor,iterations,borderType,borderValue)\n",
    " * kernel: structuring element used for dilation(created using #getStructuringElement)\n",
    " * anchor: position of anchor within element(default value(-1,-1) means anchor @ element center)\n",
    " * iterations: number of times dilation is applied.\n",
    " * borderType: pixel extrapolation method\n",
    " * borderValue: border value in case of a constant border\n",
    "\n",
    "kernel=np.ones((5,5),np.uint8)\n",
    "dilation=cv2.dilate(img,kernel,iterations=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Opening :\n",
    " * Refers to 'erosion followed by dilation'\n",
    "\n",
    "***Used***: Removing noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T17:30:08.464919Z",
     "start_time": "2021-09-04T17:29:55.818685Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@ opening=cv2.morphologyEx(src,op,kernel,anchor,iterations,borderType,borderValue)\n",
    " * op: Type of a morphological operation(here op=cv2.MORPH_OPEN)\n",
    " * kernel: Structuring element(created using #getStructuringElement)\n",
    " * anchor: Anchor position with kernel (-ve values mean that anchor @ kernel center)\n",
    " * iterations: Number of times erosion & dilation are applied.\n",
    " * borderType: pixel extrapolation method\n",
    " * borderValue: border value in case of a constant border\n",
    "\n",
    "kernel=np.ones((5,5),np.uint8)\n",
    "opening=cv2.morphologyEx(img,cv2.MORPH_OPEN,kernel)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Closing :\n",
    " * Reverse of Opening,'dilation followed by erosion'\n",
    "\n",
    "***Used*** : Closing small holes inside foreground objects|small black points on object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@ closing=cv2.morphologyEx(src,op,kernel,anchor,iterations,borderType,borderValue)\n",
    " * op: Type of a morphological operation(here op=cv2.MORPH_CLOSE)\n",
    " * kernel: Structuring element(created using #getStructuringElement)\n",
    " * anchor: Anchor position with kernel (-ve values mean that anchor @ kernel center)\n",
    " * iterations: Number of times erosion & dilation are applied.\n",
    " * borderType: pixel extrapolation method\n",
    " * borderValue: border value in case of a constant border\n",
    " \n",
    "kernel=np.ones((5,5),np.uint8)\n",
    "closing=cv2.morphologyEx(img,cv2.MORPH_CLOSE,kernel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Morphological Gradient:\n",
    " * It is difference between 'dilation' & 'erosion' of an image                               \n",
    "                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@ gradient=cv2.morphologyEx(img,cv2.MORPH_GRADIENT,kernel)                               \n",
    " * op: Type of a morphological operation(here op=cv2.MORPH_GRADIENT)\n",
    " * kernel: Structuring element(created using #getStructuringElement)\n",
    " * anchor: Anchor position with kernel (-ve values mean that anchor @ kernel center)\n",
    " * iterations: Number of times erosion & dilation are applied.\n",
    " * borderType: pixel extrapolation method\n",
    " * borderValue: border value in case of a constant border\n",
    " \n",
    ">>> kernel=np.ones((5,5),np.uint8)                                                              \n",
    ">>> gradient=cv2.morphologyEx(img,cv2.MORPH_GRADIENT,kernel) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Top Hat :\n",
    " * It is difference between 'input image' & 'Opening of image'                               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@ tophat=cv2.morphologyEx(img,cv2.MORPH_TOPHAT,kernel)\n",
    " * op: Type of a morphological operation(here op=cv2.MORPH_TOPHAT)\n",
    " * kernel: Structuring element(created using #getStructuringElement)\n",
    " * anchor: Anchor position with kernel (-ve values mean that anchor @ kernel center)\n",
    " * iterations: Number of times erosion & dilation are applied.\n",
    " * borderType: pixel extrapolation method\n",
    " * borderValue: border value in case of a constant border\n",
    "\n",
    ">>> kernel=np.ones((5,5),np.uint8)                              \n",
    ">>> tophat=cv2.morphologyEx(img,cv2.MORPH_TOPHAT,kernel)                               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Black Hat : \n",
    " * It is difference between 'closing of input image' & 'input image'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@ blackhat=cv2.morphologyEx(img,cv2.MORPH_BLACKHAT,kernel)\n",
    " * op: Type of a morphological operation(here op=cv2.MORPH_BLACKHAT)\n",
    " * kernel: Structuring element(created using #getStructuringElement)\n",
    " * anchor: Anchor position with kernel (-ve values mean that anchor @ kernel center)\n",
    " * iterations: Number of times erosion & dilation are applied.\n",
    " * borderType: pixel extrapolation method\n",
    " * borderValue: border value in case of a constant border\n",
    "\n",
    "kernel=np.ones((5,5),np.uint8)                              \n",
    "blackhat=cv2.morphologyEx(img,cv2.MORPH_BLACKHAT,kernel)\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Structuring Element & Kernel:\n",
    " * We can manually create 'structuring elements' help of Numpy(rectangular shape)\n",
    " * elliptical/circular shaped kernel\n",
    " * pass shape & size of kernel,get desired kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "@ kernel=cv2.getStructuringElement(shape,ksize,anchor)\n",
    " * ksize: Size of structuring element(row,col)\n",
    " * anchor: Anchor position within element(default (-1,-1):anchor @ center)\n",
    " * shape: Element shape \n",
    "    - cv2.MORPH_RECT: Rectangle\n",
    "    - cv2.MORPH_ELLIPSE: ellipse\n",
    "    - cv2.MORPH_CROSS :cross - shaped\n",
    "        \n",
    ">>> kernel=cv2.getStructuringElement(cv2.MORPH_RECT,(5,5))  # Rectangular Kernel\n",
    ">>> cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(5,5))      # Elliptical Kernel\n",
    ">>> cv2.getStructuringElement(cv2.MORPH_CROSS,(5,5))        # Cross-shaped Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Image Gradients: \n",
    " * OpenCV provides three types of ***gradient filters***|***High-pass filters*** <br>\n",
    "    ***Sobel X*** , ***Sobel Y*** , ***Laplacian***\n",
    "   \n",
    "https://docs.opencv.org/4.5.2/d5/d0f/tutorial_py_gradients.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Sobel & Scharr Derivatives : \n",
    " * Gausssian smoothing plus differentiation operation (more resistant to noise).\n",
    " * `Direction of derivatives to be taken(vertical|horizontal using 'yorder' & 'xorder')` \n",
    " * `'size of kernel' by ksize(If ksize=-1: '3x3 Scharr filter used, gives better results' than '3x3 Sobel filter')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-05T15:33:47.538575Z",
     "start_time": "2021-09-05T15:33:29.791214Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@ sobel=Sobel(src,ddepth,dx,dy,ksize,scale,delta,borderType)\n",
    " * ddepth: output image depth\n",
    " * dx,dy: order of derivative x & y\n",
    " * ksize: size of extended Sobel kernel(must be odd)\n",
    " * scale: optional scale factor for computed derivative values(default None)\n",
    " * delta: optional delta value that is added to  results prior to storing them in dst.\n",
    " * borderType: pixel extrapolation method\n",
    "        \n",
    "sobelx=cv2.Sobel(img,cv2.CV_64F,1,0,ksize=3)    ## Sobel X\n",
    "sobely=cv2.Sobel(img,cv2.CV_64F,0,1,ksize=3)    ## Sobel Y   \n",
    "  \n",
    "cv2.imshow(\"sx\",sobelx)\n",
    "cv2.imshow(\"sy\",sobely)\n",
    "cv2.imshow(\"img\",img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Laplacian Derivatives :\n",
    " * Calculates Laplacian of image given by relation.\n",
    " * each derivative is found using 'Sobel derivatives'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@ laplacian=cv2.Laplacian(src,ddepth,ksize,scale,delta,borderType)\n",
    " * ddepth: output image depth\n",
    " * ksize: size of extended Sobel kernel(must be odd)\n",
    " * scale: optional scale factor for computed derivative values(default None)\n",
    " * delta: optional delta value that is added to  results prior to storing them in dst.\n",
    " * borderType: pixel extrapolation method\n",
    "\n",
    "laplacian=cv2.Laplacian(img,cv2.CV_64F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Canny Edge Detection:\n",
    " * Popular 'edge detection algorithm',was developed by 'John F. Canny' in 1986 \n",
    "\n",
    "**Multi-stages of Canny :**\n",
    "  * **Noise Reduction:**\n",
    "     'remove noise' using '5x5 Gaussian filter' from image.\n",
    "                                                                                                     \n",
    "  * **Finding Intensity Gradient of the Image:** \n",
    "    * Smoothened image,filtered with 'Sobel kernel'(both horizontal & vertical direction)\n",
    "    * Which get first derivative in \"horizontal direction(Gx)\" & \"vertical direction(Gy)\" \n",
    "    * From these 2 images,we can find 'Edge_Gradient' & 'direction for each pixel' \n",
    "                 \n",
    "                    `Edge_Gradient(G) =sqrt{(Gx)^2 + (Gy)^2}`\n",
    "\n",
    "                     `Angle(theta) =tan(inv) (Gy/Gx)`\n",
    "\n",
    "    * Gradient direction is always 'perpendicular to edges'.So,rounded to one of four angles(representing vertical,horizontal & two diagonal directions)\n",
    " \n",
    "  * **Non-maximum Suppression** :\n",
    "    * After getting 'Edge_Gradient & direction',a full scan of image is done to 'remove any unwanted pixels',which maynt constitute edge\n",
    "    * At every pixel,'pixel is checked, if it is a local maximum' in its neighborhood in direction of gradient\n",
    "    * If it forms 'local maximum', it is considered for next stage, Else it is suppressed( put to 0)\n",
    "    * In short, result we get is a 'binary image with “thin edges” \n",
    "  \n",
    "  * **Hysteresis Thresholding** :\n",
    "    * This stage decides, which are all edges are really edges & which arent,that need 2 threshold values, 'minVal' & 'maxVal'. \n",
    "    * Edges with 'intensity gradient > maxVal': sure to be edges & 'intensity gradient < maxVal': sure to be non-edges(are discarded)\n",
    "    \n",
    "    * Those who lie between these 2 thresholds are classified 'edges' / 'non-edges' based on their connectivity.\n",
    "    * If they 'connected to “sure-edge” pixels': considered to be part of edges. Else they are discarded\n",
    "   * This stage, 'removes small pixels noises' on assumption that edges are long lines.So we finally, get 'strong edges' in image\n",
    "\n",
    "\n",
    "Canny Edge Detection in OpenCV :\n",
    " - OpenCV puts all the above in single function'cv2.Canny()'\n",
    " - First argument is our input image. \n",
    "     Last argument is L2gradient which specifies the equation for finding gradient magnitude. If it is True,\n",
    "        it uses the equation mentioned above which is more accurate,\n",
    "otherwise it uses this function: . By default, it is False.\n",
    "\n",
    "\n",
    "https://docs.opencv.org/4.5.2/da/d22/tutorial_py_canny.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-05T15:57:39.773847Z",
     "start_time": "2021-09-05T15:57:33.899786Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@ edge=cv2.Canny(image,threshold1,threshold2,edges,apertureSize,L2gradient)\n",
    " * image:\n",
    " * threshold1:'minVal threshold'\n",
    " * threshold2:'maxVal threshold'\n",
    " * edges:output edge map, single channels 8-bit image, which has same size as image\n",
    " * apertureSize: 'Size of Sobel kernel' used for find image gradients (default = 3)\n",
    " * L2gradient: Specifies equation for finding Edge_Gradient(G)=|Gx|+|Gy| If True. (Default False) \n",
    "\n",
    "edge=cv2.Canny(img,threshold1=100,threshold2=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Image Pyramids\n",
    " * when, we need to work with images of different resolution of same image\n",
    " * These set of images(different resolution) called 'Image Pyramids',cuz `biggest image kept @ bottom & smallest image @ top` \n",
    " * Two kinds of Image Pyramids: (1) Gaussian Pyramid  (2) Laplacian Pyramids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Gaussian Pyramid:    \n",
    " * 'Higher level(Low resolution)' formed by removing consecutive 'rows' & 'columns' in 'Lower level(higher resolution)' image \n",
    " * Then each pixel in 'higher level',formed by contribution from 5 pixels in underlying level with gaussian weights.\n",
    " * 'M*N' image becomes M/2 * N/2 image(area reduces to one-fourth of original area),called 'Octave' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\" CV2.pyrDown() \"\"\" :Lower Resolution \n",
    "@ lower=pyrDown(src,dstsize,borderType)\n",
    "\n",
    "low=cv2.pyrDown(img)\n",
    "low1=cv2.pyrDown(low)\n",
    "low2=cv2.pyrDown(low1)\n",
    " \n",
    "\"\"\" CV2.pyrUp() \"\"\" :Higher Resolution '\n",
    "@ upper=pyrUp(src,dstsize,borderType)\n",
    "up=cv2.pyrUp(img)\n",
    "up1=cv2.pyrUp(up)\n",
    "up2=cv2.pyrUp(up1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "####  Laplacian Pyramid :\n",
    " * Formed from 'Gaussian Pyramids'.Laplacian pyramid images are like edge images only(Most elements are zeros)\n",
    " * Laplacian Pyramid is formed by difference between that level in Gaussian Pyramid & expanded version of its upper level in Gaussian Pyramid. \n",
    " * Used in image compression.\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Contours :\n",
    "* Curve joining all continuous points(along boundary), having same color|intensity\n",
    "* Usefull for `'shape analysis','object detection' & 'recognition'`\n",
    "* For before finding contours,`apply threshold/canny edge detection`(for better accuracy)\n",
    "\n",
    "***Note :***\n",
    "* Contours detection like finding white object from black background, `Object to be found: in white & backgroud: in black`\n",
    "* `findContours() no longer modifies source image`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Find Contours : Using 'cv2.findContours()'.\n",
    "\n",
    "**Requirement & Steps(for finding contours) :**\n",
    "     * Convet image to Gray_Scale      * Thresholding     * cv2.findContours()\n",
    "\n",
    "**Returns :**\n",
    "* Contours: Python list of all contours in image (each contour is nd-array of (x,y) coordinates of boundary points of object)\n",
    "* hierarchy: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "@ contours,hierarchy=cv2.findContours(image,mode,method,contours,hierarchy,offset)\n",
    "   * mode: Contour retrieval mode\n",
    "      - cv2.RETR_TREE:\n",
    "   * method: Contour approximation method\n",
    "      -cv2.CHAIN_APPROX_NONE : All boundary points are stored\n",
    "      -cv2.CHAIN_APPROX_SIMPLE : Removes all redundant points & compresses contour(saves memory) \n",
    "   * contours:\n",
    "   * hierarchy:\n",
    "   * offset: Optional offset by which every contour point is shifted\n",
    "\n",
    "img_gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)    ## img to Gray_Scale\n",
    "ret,thresh=cv2.threshold(img_gray,127,255,0)     ## Threshold Of Image\n",
    "contours,hierarchy=cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)  ## Find Contour\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Draw Contours : Using 'cv2.drawContours()' \n",
    "* Used to draw any shape provided you have its boundary points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@ drawContours(image,contours,contourIdx,color,thickness,lineType,hierarchy,maxLevel,offset)\n",
    "  * image: Destination image.\n",
    "  * contours: All input contours(Each contour stored as point vector)\n",
    "  * contourIdx: Which contour to draw(If -1: draw all contour Else specific contour)\n",
    "  * color: Color of contours(in form of range(like, green=(0,255,255)))\n",
    "  * thickness: Thickness of lines, contours are drawn \n",
    "  * lineType: Line connectivity\n",
    "  * hierarchy: Optional information about hierarchy\n",
    "  * maxLevel: Maximal level for drawn contours\n",
    "  * offset: Optional contour shift parameter. Shift all drawn contours by specified space\n",
    "    \n",
    "img=cv2.drawContours(img,contours,-1,(150,25,255),3)  # To draw all contours in image\n",
    "\n",
    "cv.drawContours(img,contours,3,(0,255,0),3)    # To draw specific contour (ex: 4th contour) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Contour Features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Moments \"\"\"\n",
    "- Weighted average of image pixel intensities\n",
    "- Usefull for describe area(total intensity) , centroid & its orientation\n",
    "-'cv2.moments()' gives a dictionary of all moment values calculated\n",
    "\n",
    "contours,hierarchy=cv.findContours(thresh,1,2)\n",
    "m=cv2.moments(contours[0])\n",
    " \n",
    "# Contour Centroid:\n",
    "cx=int(m['m10']/m['m00'])\n",
    "cy=int(m['m01']/m['m00'])\n",
    "    \n",
    "# Contour Area: \n",
    "@ cv2.contourArea(contour,oriented)\n",
    " * contour: contour Input vector of 2D points\n",
    " * oriented: Oriented area flag(If True,function returns 'signed area value',depending on contour orientation)\n",
    "\n",
    "area=cv2.contourArea(contours[2])  \n",
    "\n",
    "# Contour Perimeter:\n",
    "@ perimeter=cv2.arcLength(curve,closed)\n",
    "  * curve: Input vector \n",
    "  * closed: Flag indicating whether curve is closed / not\n",
    "\n",
    "perimeter=cv2.arcLength(contours[1],True)     \n",
    "\n",
    "\n",
    "\"\"\" Contour Approximation \"\"\"\n",
    "- Approximates 'contour shape' to 'another shape' with less no. of vertices (depending on precision)\n",
    "\n",
    "epsilon=0.1*cv.arcLength(contours[3],True)\n",
    "approx=cv.approxPolyDP(contours[3],epsilon,True)\n",
    "    \n",
    " \n",
    "\"\"\" Convex Hull \"\"\"\n",
    "- Checks curve for 'convexity defects' & 'corrects it'\n",
    "- Convex curves: Always bulged out or at-least flat  ;  Convexity defects: if it is bulged inside    \n",
    "@ hull=v2.convexHull(points,hull,clockwise,returnPoints)\n",
    "  * points: contours we pass into.\n",
    "  * hull: output, normally we avoid it.\n",
    "  * clockwise : Orientation flag(If True:output convex hull is oriented clockwise)\n",
    "  * returnPoints : If True:returns coordinates of  hull points(If False:returns indices of contour points corresponding to hull points)\n",
    "\n",
    "hull=cv2.convexHull(Contours[5])\n",
    "\n",
    "\n",
    "\"\"\" Checking Convexity\"\"\"\n",
    "- Process to check, if curve is convex or not\n",
    "-'cv2.isContourConvex()' return whether True|False\n",
    "@ cv2.isContourConvex(Contours)\n",
    "    \n",
    "c=cv2.isContourConvex(Contours[2])\n",
    "\n",
    "\n",
    "\"\"\" Bounding Rectangle \"\"\"\n",
    "# Bounding with Straight Rectangle:\n",
    "- Straight rectangle & doesnot consider rotation of object.\n",
    "- So area of bounding rectangle won not be minimum\n",
    "\n",
    "x,y,w,h=cv2.boundingRect(contours[10]) ## bounding specific contour\n",
    "img=cv2.rectangle(img,(x,y),(x+w,y+h),(0,0,0),2) \n",
    " \n",
    "# Bounding with Rotated Rectangle :\n",
    "- Bounding rectangle is drawn with minimum area & considers rotation also\n",
    "\n",
    "rect=cv.minAreaRect(cnt)\n",
    "box=cv.boxPoints(rect)\n",
    "box=np.int0(box)\n",
    "cv.drawContours(img,[box],0,(0,0,255),2)\n",
    "\n",
    "\n",
    "\"\"\" Minimum Enclosing Circle \"\"\"\n",
    "- Circle which completely covers object with minimum area\n",
    "\n",
    "(x,y),radius=cv.minEnclosingCircle(cnt)\n",
    "center=(int(x),int(y))\n",
    "radius=int(radius)\n",
    "cv.circle(img,center,radius,(0,255,0),2)\n",
    "\n",
    "\n",
    "\"\"\" Fitting Ellipse \"\"\"  Fits ellipse to object\n",
    "- Returns rotated rectangle in which ellipse is inscribed\n",
    "\n",
    "ellipse=cv.fitEllipse(cnt)\n",
    "cv.ellipse(img,ellipse,(0,255,0),2)\n",
    "\n",
    "\n",
    "\"\"\" Fitting Line \"\"\" Fit line to set of points\n",
    "- Image contains set of white points &  We can approximate straight line to it\n",
    "\n",
    "rows,cols=img.shape[:2]\n",
    "[vx,vy,x,y]=cv.fitLine(cnt,cv.DIST_L2,0,0.01,0.01)\n",
    "lefty=int((-x*vy/vx)+y)\n",
    "righty=int(((cols-x)*vy/vx)+y)\n",
    "cv.line(img,(cols-1,righty),(0,lefty),(0,255,0),2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Contour Properties :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Aspect Ratio \"\"\"  Ratio of width to height of bounding rect of object.\n",
    "Formula: AspectRatio=Width/Height\n",
    "    \n",
    "x,y,w,h=cv.boundingRect(cnt)\n",
    "aspect_ratio=float(w)/h\n",
    "\n",
    "\n",
    "\"\"\" Extent \"\"\" Ratio of contour area to bounding rectangle area.\n",
    "Formula: Extent=Object Area/Bounding Rectangle Area\n",
    "    \n",
    "area=cv.contourArea(cnt)\n",
    "x,y,w,h=cv.boundingRect(cnt)\n",
    "rect_area=w*h\n",
    "extent=float(area)/rect_area\n",
    "\n",
    "\n",
    "\"\"\" Solidity \"\"\" Ratio of contour area to its convex hull area\n",
    "Formula: Solidity=Contour Area/Convex Hull Area\n",
    "\n",
    "area=cv.contourArea(cnt)\n",
    "hull=cv.convexHull(cnt)\n",
    "hull_area=cv.contourArea(hull)\n",
    "solidity=float(area)/hull_area\n",
    "\n",
    "\n",
    "\"\"\" Equivalent Diameter \"\"\" Diameter of circle whose area is same as contour area\n",
    "Formula: Equivalent Diameter= √[(4×ContourArea)/π]\n",
    "\n",
    "area=cv.contourArea(cnt)\n",
    "equi_diameter=np.sqrt(4*area/np.pi)\n",
    "\n",
    "\n",
    "\"\"\" Orientation \"\"\" Angle at which object is directed\n",
    "(x,y),(MA,ma),angle=cv.fitEllipse(contours[4])\n",
    "\n",
    "\n",
    "\"\"\" Mask & Pixel Points \"\"\" All points which comprises that object\n",
    "\n",
    "mask=np.zeros(imgray.shape,np.uint8)\n",
    "cv.drawContours(mask,[cnt],0,255,-1)\n",
    "pixelpoints=np.transpose(np.nonzero(mask))\n",
    "#pixelpoints=cv.findNonZero(mask)\n",
    "\n",
    "\n",
    "\"\"\" Maximum Value,Minimum Value & their locations \"\"\" Find parameters using mask image.\n",
    "\n",
    "min_val,max_val,min_loc,max_loc=cv.minMaxLoc(imgray,mask=mask)\n",
    "\n",
    "\n",
    "\"\"\" Mean Color or Mean Intensity \"\"\"\n",
    "- Find average color of object Or it can be average intensity of object in grayscale mode\n",
    "\n",
    "mean_val=cv.mean(im,mask=mask)\n",
    "\n",
    "\n",
    "\"\"\" Extreme Points \"\"\"  topmost,bottommost,rightmost & leftmost points of object\n",
    "\n",
    "leftmost = tuple(cnt[cnt[:,:,0].argmin()][0])\n",
    "rightmost = tuple(cnt[cnt[:,:,0].argmax()][0])\n",
    "topmost = tuple(cnt[cnt[:,:,1].argmin()][0])\n",
    "bottommost = tuple(cnt[cnt[:,:,1].argmax()][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Contour Function :\n",
    "https://docs.opencv.org/4.5.2/d5/d45/tutorial_py_contours_more_functions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Convexity Defects \"\"\" \n",
    "\n",
    "hull=cv.convexHull(cnt,returnPoints=False)\n",
    "defects=cv.convexityDefects(cnt,hull)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Contours Hierarchy\n",
    "https://docs.opencv.org/4.5.2/d9/d8b/tutorial_py_contours_hierarchy.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Histograms in OpenCV \n",
    "* Graph,that shows `intensity distribution` of image\n",
    "* `x-axis contains pixel values` & `y-axis contains pixel-range` \n",
    " - histogram provides, intuition about 'contrast','brightness','intensity distribution' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-12T18:39:45.350559Z",
     "start_time": "2021-09-12T18:39:45.326002Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function calcHist:\n",
      "\n",
      "calcHist(...)\n",
      "    calcHist(images, channels, mask, histSize, ranges[, hist[, accumulate]]) -> hist\n",
      "    .   @overload\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Using cv2.calcHist() \"\"\"\n",
    "@ calcHist(images,channels,mask,histSize,ranges,hist,accumulate)\n",
    " * images: source image (type uint8|float32)\n",
    " * channels: Index of channel for which we calculate histogram.\n",
    "    [0]:grayscal ; color image,[0]:Blue [1]:Green [2]:Red \n",
    " * mask : mask image\n",
    " * histSize : Represents BIN count\n",
    " * ranges : RANGE\n",
    "    \n",
    "hist=cv2.calcHist([img],channels=[0],mask=None,histSize=[256],ranges=[0,256])\n",
    "plt.plot(hist);plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Using np.histogram() \"\"\"\n",
    "# For Gray-scale :\n",
    "hist,bins=np.histogram(img.ravel(),histSize=256,range=[0,256])\n",
    "plt.plot(hist);plt.show()\n",
    "\n",
    "# For Color-Channel :\n",
    "hist_r,bins=np.histogram(r.ravel(),256,range=[0,256])\n",
    "hist_g,bins=np.histogram(g.ravel(),256,range=[0,256])\n",
    "hist_b,bins=np.histogram(b.ravel(),256,range=[0,256])\n",
    "\n",
    "plt.plot(hist_r,color=\"r\")\n",
    "plt.plot(hist_g,color=\"g\")\n",
    "plt.plot(hist_b,color=\"b\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Using Matplotlib\"\"\"\n",
    "# For Gray-scale :\n",
    "plt.hist(img.ravel(),256,range=[0,256]); plt.show()\n",
    "\n",
    "# For Color-Channel :\n",
    "plt.hist(r.ravel(),256,range=[0,256])\n",
    "plt.hist(g.ravel(),256,range=[0,256])\n",
    "plt.hist(b.ravel(),256,range=[0,256])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-12T19:36:19.241013Z",
     "start_time": "2021-09-12T19:36:15.448651Z"
    },
    "hidden": true
   },
   "source": [
    "#### Histogram Equalization\n",
    "https://docs.opencv.org/4.5.2/d5/daf/tutorial_py_histogram_equalization.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "####  2D Histograms\n",
    "https://docs.opencv.org/4.5.2/dd/d0d/tutorial_py_2d_histogram.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Histogram Backprojection\n",
    "https://docs.opencv.org/4.5.2/dc/df6/tutorial_py_histogram_backprojection.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Template Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k) \" Template Matching \"\n",
    "Process of searching & finding, location of 'template image' in larger image \n",
    "\n",
    "-'cv2.matchTemplate()':It simply slides template over image(as in 2D convolution) & compares template & patch image under template image.\n",
    "- Returns a grayscale image, where each pixel denotes how much does the neighbourhood of that pixel match with template.\n",
    "#@ tm=cv2.matchTemplate(image,templ,method,result,mask)\n",
    " * image: larger image(must be 8-bit / 32-bit)\n",
    " * templ: template image(mustnt > source_img & same data type)\n",
    " * result: Map of comparison results(o/p)\n",
    " * method: type of template maching mode\n",
    "    {cv2.TM_CCOEFF,cv2.TM_CCOEFF_NORMED,cv2.TM_CCORR,cv2.TM_CCORR_NORMED,cv2.TM_SQDIFF,cv2.TM_SQDIFF_NORMED}\n",
    " * mask: Mask of searched template. It must have the same datatype and size with templ\n",
    "    \n",
    "-If input image is of size(WxH) & template image is of size(wxh),output image will have size(W-w+1, H-h+1)\n",
    "-After getting templatematch('tm'),we can use 'cv2.minMaxLoc()' to find where is 'max_value & min_value'. \n",
    "#@ min_val,max_val,min_loc,max_loc=cv2.minMaxLoc(src,mask)\n",
    " * src: input single-channel array.\n",
    " * mask: optional mask used to select a sub-array\n",
    " * minVal: pointer to the returned minimum value; NULL is used if not required.\n",
    " * maxVal: pointer to the returned maximum value; NULL is used if not required.\n",
    " * minLoc: pointer to the returned minimum location (in 2D case); NULL is used if not required.\n",
    " * maxLoc: pointer to the returned maximum location (in 2D case); NULL is used if not required.\n",
    "\n",
    "-By Takeing 'max_value & min_value' as 'top-left corner of rectangle' & take template(w,h) as width & height of rectangle.\n",
    "-That is helpful for drawing rectangle in region of template.\n",
    "\n",
    "##### matching Single template \n",
    "import cv2\n",
    "import numpy as np\n",
    "img=cv2.imread(r\"A:\\Python_projects\\tianic_kaggle\\images\\titanic-disaster.jpg\")\n",
    "img_gr=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "temp=cv2.imread(r\"C:\\Users\\Sud()\\Desktop\\OCV_fle\\temp.jpg\",0)\n",
    "\n",
    "w,h=temp.shape[::-1]   ## provide (w,h) of template\n",
    "tm=cv2.matchTemplate(img_gr,temp,cv2.TM_CCOEFF_NORMED)   ## template matching\n",
    "\n",
    "min_val,max_val,min_loc,max_loc=cv2.minMaxLoc(tm)  ## \n",
    "\n",
    "# pt1=min_loc    ## for method:{cv2.TM_SQDIFF,cv2.TM_SQDIFF_NORMED} (pt1 for top_left points)\n",
    "pt1=max_loc      ## for remaining all  (pt1 for top_left points)\n",
    "pt2=(pt1[0]+w, pt1[1]+h)  ## (pt2 for bottom_right points)\n",
    "\n",
    "cv2.rectangle(img,pt1,pt2,(0,0,255),1)  ## setting up rectangle\n",
    "\n",
    "cv2.imshow('detected',img)\n",
    "#cv2.imshow('template',temp)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows() \n",
    "\n",
    "\n",
    "##### Matching multiple Templates\n",
    "import cv2\n",
    "import numpy as np\n",
    "img=cv2.imread(r\"A:\\Python_projects\\tianic_kaggle\\images\\titanic-disaster.jpg\")\n",
    "img_gr=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "temp=cv2.imread(r\"C:\\Users\\Sud()\\Desktop\\OCV_fle\\temp.jpg\",0)\n",
    "\n",
    "w,h=temp.shape[::-1]   ## provide (w,h) of template\n",
    "tm=cv2.matchTemplate(img_gr,temp,cv2.TM_CCOEFF_NORMED)   ## template matching\n",
    "\n",
    "threshold=0.95       ## setting threshold value\n",
    "loc=np.where(tm >= threshold)  ## greater than threshold value\n",
    "\n",
    "for pt1 in zip(*loc[::-1]):     ## finding locations of 'templates' in image (location (w,h))\n",
    "    cv2.rectangle(img,pt1,(pt1[0]+w,pt1[1]+h),(0,0,0),1) ##(W-w+1, H-h+1)\n",
    "\n",
    "cv2.imshow('detected',img)  ## matched image(template patched over image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l) Hough Transform\n",
    "\n",
    "\n",
    "a) Hough Line Transformation\n",
    "\n",
    "\n",
    "\n",
    "b) Hough Line Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img=cv2.imread(r\"C:\\Users\\Sud()\\Desktop\\OCV_fle\\lines.PNG\")\n",
    "#img_gray=cv2.imread(r\"C:\\Users\\Sud()\\Desktop\\OCV_fle\\lines.PNG\",0)\n",
    "cv2.namedWindow('e1',cv2.WINDOW_NORMAL)\n",
    "#cv2.namedWindow('e2',cv2.WINDOW_NORMAL)\n",
    "\n",
    "e1 = cv2.Canny(img,50,150,apertureSize = 3)\n",
    "#e2 = cv2.Canny(img_gray,50,150,apertureSize = 3)\n",
    "\n",
    "lines=cv2.HoughLines(e1,1,np.pi/180,50)\n",
    "for rho,theta in lines[0]:\n",
    "    a = np.cos(theta)\n",
    "    b = np.sin(theta)\n",
    "    x0 = a*rho\n",
    "    y0 = b*rho\n",
    "    x1 = int(x0 + 1000*(-b))\n",
    "    y1 = int(y0 + 1000*(a))\n",
    "    x2 = int(x0 - 1000*(-b))\n",
    "    y2 = int(y0 - 1000*(a))\n",
    "\n",
    "    cv2.line(img,(x1,y1),(x2,y2),(0,0,255),2)\n",
    "\n",
    "\n",
    "\n",
    "#cv2.imshow('e2',e2)\n",
    "cv2.imshow('detected',img)\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines=cv2.HoughLines(image,rho,theta,threshold,lines,srn,stn,min_theta,max_theta)\n",
    "*image : 8-bi (single-channel binary)\n",
    "* rho : \n",
    "* theta \n",
    "* threshold \n",
    "* lines \n",
    "* srn \n",
    "* stn \n",
    "* min_theta \n",
    "* max_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Feature Detection & Description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Machine Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  7) Computational Photography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Camera Calibration & 3D Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Video Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  OpenCV-Python Bindings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T19:23:47.478124Z",
     "start_time": "2021-09-06T19:23:47.461799Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2279110502512, 2279109008336)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img2=img.copy()\n",
    "id(img),id(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T19:23:13.246889Z",
     "start_time": "2021-09-06T19:23:13.230440Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2279110502512, 2279110502512)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## copy image\n",
    "img2=img.copy()\n",
    "id(img),id(img2)  # different memory loaction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flipping\n",
    "@ flip(src,flipCode,dst)\n",
    " * src: input array.\n",
    " * dst: output (array of same size & type as src)\n",
    " * flipCode: flag to specify how to flip array\n",
    "     - flipcode = 0: flip vertically around 'x-axis'\n",
    "     - flipcode > 0: flip horizontally around 'y-axis' +ve value\n",
    "     - flipcode&#60 0: flip vertically and horizontally, flipping around both axes negative value\n",
    "        \n",
    "flip=cv.flip(img,-1)\n",
    "\n",
    "\n",
    "\n",
    "# Cropping\n",
    "cropped = img[200:400, 300:400]\n",
    "\n",
    "\n",
    "\n",
    "# different RGB colors and assign them to the variable\n",
    "blue, green, red = baboon[:, :, 0], baboon[:, :, 1], baboon[:, :, 2]\n",
    "\n",
    "## Show Image \n",
    "cv2.imshow('B',blue)\n",
    "cv2.imshow('R',red)\n",
    "cv2.imshow('G',green)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "baboon_red = baboon.copy()\n",
    "baboon_red[:, :, 0] = 0\n",
    "baboon_red[:, :, 1] = 0\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(cv2.cvtColor(baboon_red, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "baboon_blue = baboon.copy()\n",
    "baboon_blue[:, :, 1] = 0\n",
    "baboon_blue[:, :, 2] = 0\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(cv2.cvtColor(baboon_blue, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "baboon_green = baboon.copy()\n",
    "baboon_green[:, :, 0] = 0\n",
    "baboon_green[:, :, 2] = 0\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(cv2.cvtColor(baboon_green, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3|iii) Performance Measurement and Improvement Techniques\n",
    "- In image processing,we are dealing with large no. of operations per second\n",
    "- It is mandatory, providing correct solution in 'fastest manner'\n",
    " * To measure performance of code\n",
    " * Tips to improve performance of code\n",
    "\n",
    "                      a) Performance Measure\n",
    "\n",
    "##### Performance Measuring with OpenCV\n",
    "*'cv2.getTickCount'returns 'no. of clock-cycles after event A to event B'\n",
    "  - we can call it, before(event A) & after(event B) of the function execution(get 'clock-cycles' used to execute function)\n",
    "\n",
    "*'cv2.getTickFrequency' returns 'frequency of clock-cycles / no. of clock-cycles per secs',that find time of execution in secs \n",
    "\n",
    ">>> event_A=cv2.getTickCount() ## before code Execution\n",
    "## your code execution\n",
    ">>> event_B=cv2.getTickCount() ## after code Execution         \n",
    ">>> time=(event_A - event_B)/cv2.getTickFrequency()\n",
    ">>> time \n",
    "\n",
    "\n",
    "##### Performance Measuring with IPython\n",
    "\n",
    "\n",
    "                         b) Optimization \n",
    "\n",
    "##### Optimizationin OpenCV\n",
    "- Many OpenCV functions are optimized using 'SSE2','AVX'.Also contains unoptimized code also.\n",
    "- OpenCV runs optimized code if it is enabled, else it runs the unoptimized code.\n",
    "- We can use 'cv2.useOptimized()' to check if it is enabled/disabled & 'cv2.setUseOptimized()' to enable/disable it.\n",
    "\n",
    ">>> cv2.useOptimized() ## check if optimization is enabled\n",
    ">>> cv2.setUseOptimized(False)  ## Disable it\n",
    "                                                                                                                                                                                                                                                                                                                                                                   \n",
    "\n",
    "\n",
    "                              c) Performance Optimization Techniques\n",
    "\n",
    "1) Avoid 'loops in Python' as possible,like double/triple loops etc(They are inherently slow)\n",
    "2) 'Vectorize algorithm/code' at max possible extent because Numpy & OpenCV are optimized for vector operations.\n",
    "3) Exploit cache coherence.\n",
    "4) 'Never make copies' of array unless needed(Try to use views instead as 'Array copying is costly operation') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image blending \n",
    "https://docs.opencv.org/4.5.2/dc/dff/tutorial_py_pyramids.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
